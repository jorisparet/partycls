<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>partycls.clustering API documentation</title>
<meta name="description" content="Clustering algorithms." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>partycls.clustering</code></h1>
</header>
<section id="section-intro">
<p>Clustering algorithms.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Clustering algorithms.
&#34;&#34;&#34;

import numpy
from sklearn.cluster import KMeans as _KMeans
from sklearn.mixture import GaussianMixture as _GaussianMixture
from partycls.descriptor import StructuralDescriptor, DummyDescriptor, BondOrientationalDescriptor

__all__ = [&#39;KMeans&#39;, &#39;GaussianMixture&#39;, &#39;CommunityInference&#39;]


class Clustering:
    &#34;&#34;&#34;
    Base class (abstract) for clustering methods.
    
    Parameters
    ----------
    
    n_clusters : int, optional
        Requested number of clusters. The default is 2.
    
    n_init : int, optional
        Number of times the clustering will be run with different seeds. 
        The default is 1.
            
    Attributes
    ----------

    n_clusters : int, optional
        Number of clusters.
    
    n_init : int, optional
        Number of times the clustering is run.
    
    labels : list of int
        Cluster labels. The default is None. Initialized after the `fit`
        method was called.
        
    backend : compatible backend
        Backend used for the clustering method if it relies on an external
        package.
   
    &#34;&#34;&#34;

    def __init__(self, n_clusters=2, n_init=1):
        self.n_clusters = n_clusters
        self.n_init = n_init
        self.labels = None
        self.backend = None

    def __str__(self):
        rep = &#39;Clustering(method=&#34;{}&#34;, n_clusters={}, n_init={})&#39;
        return rep.format(self.full_name, self.n_clusters, self.n_init)

    def __repr__(self):
        return self.__str__()

    def fit(self, X):
        pass

    @property
    def fractions(self):
        if self.labels is not None:
            f_k = numpy.empty(self.n_clusters, dtype=numpy.float64)
            for k in range(self.n_clusters):
                f_k[k] = numpy.sum(self.labels == k)
            return f_k / len(self.labels)

    @property
    def populations(self):
        if self.labels is not None:
            n_k = numpy.empty(self.n_clusters, dtype=numpy.int64)
            for k in range(self.n_clusters):
                n_k[k] = numpy.sum(self.labels == k)
            return n_k

    def centroids(self, X):
        &#34;&#34;&#34;
        Central feature vector of each cluster.
        
        Each object in the dataset over which the clustering was performed is 
        assigned a discrete label. This label represents the index of the 
        nearest cluster center to which this object belongs. The centroid (i.e. 
        the cluster center), is thus the average feature vector of all the 
        objects in the cluster.
        
        Cluster memberships of the objects are stored in the `labels`
        attribute. Coordinates of the centroids can then be calculated for an
        arbitrary dataset `X`, provided it has the same shape as the original 
        dataset used for the clustering.

        Parameters
        ----------
        X : numpy.ndarray
            Array of features (dataset) for which to compute the centroids.

        Returns
        -------
        C_k : numpy.ndarray
            Cluster centroids. C_k[n] is the coordinates of the n-th cluster 
            center.

        &#34;&#34;&#34;
        n_features = X.shape[1]
        C_k = numpy.zeros((self.n_clusters, n_features), dtype=numpy.float64)
        n_k = self.populations
        for k in range(self.n_clusters):
            for n in range(len(self.labels)):
                if self.labels[n] == k:
                    C_k[k] += X[n]
            C_k[k] = C_k[k] / n_k[k]
        return C_k


class KMeans(Clustering):
    &#34;&#34;&#34;
    KMeans clustering.
    
    This class relies on the class `KMeans` from the machine learning package 
    &#34;scikit-learn&#34;. An instance of sklearn.cluster.KMeans is created when 
    calling the `fit` method, and is then accessible through the `backend`
    attribute for later use. See scikit&#39;s documentation for more information on
    the original class.
    &#34;&#34;&#34;

    def __init__(self, n_clusters=2, n_init=1):
        self.symbol = &#39;kmeans&#39;
        self.full_name = &#39;K-Means&#39;
        Clustering.__init__(self, n_clusters=n_clusters, n_init=n_init)

    def fit(self, X):
        &#34;&#34;&#34;
        Run the K-Means algorithm on `X`.
        The predicted labels are updated in the attribute `labels` of 
        the current instance of `KMeans`.
        &#34;&#34;&#34;
        self.backend = _KMeans(n_clusters=self.n_clusters,
                               n_init=self.n_init)
        if hasattr(X, &#39;features&#39;):
            self.backend.fit(X.features)
        else:
            self.backend.fit(X)
        self.labels = self.backend.labels_


class GaussianMixture(Clustering):
    &#34;&#34;&#34;
    Gaussian Mixture.
    
    This class relies on the class `GaussianMixture` from the machine learning 
    package &#34;scikit-learn&#34;. An instance of sklearn.mixture.GaussianMixture is 
    created when calling the `fit` method, and is then accessible through the 
    `backend` attribute for later use. See scikit&#39;s documentation for more 
    information on the original class.
    &#34;&#34;&#34;

    def __init__(self, n_clusters=2, n_init=1):
        self.symbol = &#39;gmm&#39;
        self.full_name = &#39;Gaussian Mixture&#39;
        Clustering.__init__(self, n_clusters=n_clusters, n_init=n_init)

    def fit(self, X):
        &#34;&#34;&#34;
        Run the EM algorithm on `X` using a mixture of Gaussians.
        The predicted labels are updated in the attribute `labels` of the 
        current instance of `GaussianMixture`.
        &#34;&#34;&#34;
        self.backend = _GaussianMixture(n_components=self.n_clusters,
                                        n_init=self.n_init)
        if hasattr(X, &#39;features&#39;):
            self.backend.fit(X.features)
        else:
            self.backend.fit(X)
        self.labels = self.backend.predict(X)


class CommunityInference(Clustering):
    &#34;&#34;&#34;
    Community Inference is a hard clustering method based on information 
    theory. See &#34;Paret et al. https://doi.org/10.1063/5.0004732&#34; for more 
    details.
    &#34;&#34;&#34;

    def __init__(self, n_clusters=2, n_init=1):
        self.symbol = &#39;cinf&#39;
        self.full_name = &#39;Community Inference&#39;
        Clustering.__init__(self, n_clusters=n_clusters, n_init=n_init)
        self.mutual_information = None

    def fit(self, X):
        &#34;&#34;&#34;
        Community inference algorithm.
        &#34;&#34;&#34;
        if isinstance(X, StructuralDescriptor):
            descriptor = X
        else:
            descriptor = DummyDescriptor()
            features = numpy.empty_like(X)
            bins = X.shape[1]
            for n in range(features.shape[0]):
                features[n] = numpy.histogram(X[n], bins=bins)[0]
            descriptor.features = features

        MI_previous, labels_previous = self._inference_loop(descriptor)
        for n in range(self.n_init - 1):
            MI_next, labels_next = self._inference_loop(descriptor)
            # optimization `n` is worse than the previous one
            if MI_next &lt; MI_previous:
                self.mutual_information = MI_previous
                self.labels = labels_previous
            # optimization `n` is better than the previous one
            # it becomes the new standard
            else:
                MI_previous = MI_next
                labels_previous = labels_next

    def _inference_loop(self, descriptor):

        import random

        # shortcuts
        N = descriptor.size
        K = self.n_clusters
        Km1 = K - 1  # loop invariant

        # randomly set the labels
        self.labels = numpy.array([random.randint(0, Km1) for n in range(N)])
        # populations and fractions
        n_k = self.populations
        f_k = self.fractions
        # community histograms
        dtype = descriptor.features.dtype
        H_k = numpy.zeros((K, descriptor.n_features), dtype=dtype)
        for i in range(N):
            k_i = self.labels[i]
            H_k[k_i] += descriptor.features[i]
        # community distributions
        P_k = numpy.empty_like(H_k, dtype=numpy.float64)
        for k in range(K):
            P_k[k] = descriptor.normalize(H_k[k] / n_k[k])
        # average distribution
        P_average = f_k @ P_k

        # community information
        if isinstance(descriptor, BondOrientationalDescriptor):
            # in case of non-successive values of l in the grid
            dx = 1.0
        else:
            dx = descriptor.grid[1] - descriptor.grid[0]

        def _mutual_information(P_average, P_k, f_k, dx):
            MI = 0.0
            f_x = numpy.empty_like(P_average)
            for k in range(len(f_k)):
                for m in range(len(P_average)):
                    if P_k[k, m] != 0.0 and P_average[m] != 0.0:
                        f_x[m] = P_k[k, m] * numpy.log2(P_k[k, m] / P_average[m])
                    else:
                        f_x[m] = 0.0

#                #TODO: is it more efficient?
#                f_x = P_k[k] * (numpy.log2(P_k[k],
#                         out=numpy.zeros_like(P_average),
#                         where=(P_k[k] != 0.0))
#                         - numpy.log2(P_average,
#                         out=numpy.zeros_like(P_average),
#                         where=(P_average != 0.0)))

                f_x = f_x * f_k[k]
                MI = MI + numpy.sum(f_x) * dx
            return MI
        self.mutual_information = _mutual_information(P_average, P_k, f_k, dx)

        # Begin loop
        no_change = 0
        while no_change &lt; N:
            # sample particles
            sample = random.sample(range(N), N)
            for i in sample:
                k_i = self.labels[i]
                k_others = [k for k in range(K) if k != k_i]
                # lists to store attemps for relocation of `i`
                labels_list = [numpy.empty_like(self.labels) for k in range(Km1)]
                n_k_list = [numpy.empty_like(n_k) for k in range(Km1)]
                H_k_list = [numpy.empty_like(H_k) for k in range(Km1)]
                P_k_list = [numpy.empty_like(P_k) for k in range(Km1)]
                I_list = [0.0 for k in range(Km1)]
                # loop over all other available communnities
                for kn, k_j in enumerate(k_others):
                    labels_new = self.labels.copy()
                    n_k_new = n_k.copy()
                    H_k_new = H_k.copy()
                    P_k_new = P_k.copy()
                    # k_i --&gt; k_j
                    labels_new[i] = k_j
                    labels_list[kn] = labels_new
                    # changes in populations and fractions
                    n_k_new[k_i] -= 1
                    n_k_new[k_j] += 1
                    n_k_list[kn] = n_k_new
                    f_k_new = n_k_new / N
                    # changes in histograms
                    H_k_new[k_i] -= descriptor.features[i]
                    H_k_new[k_j] += descriptor.features[i]
                    H_k_list[kn] = H_k_new
                    # changes in distributions
                    P_k_new[k_i] = descriptor.normalize(H_k_new[k_i] / n_k_new[k_i])
                    P_k_new[k_j] = descriptor.normalize(H_k_new[k_j] / n_k_new[k_j])
                    P_k_list[kn] = P_k_new
                    # change in community information
                    I_new = _mutual_information(P_average, P_k_new, f_k_new, dx)
                    I_list[kn] = I_new
                # find the most appropriate community for `i`
                best = numpy.argmax(I_list)
                if I_list[best] &gt; self.mutual_information:
                    self.labels = labels_list[best]
                    self.mutual_information = I_list[best]
                    n_k = n_k_list[best]
                    H_k = H_k_list[best]
                    P_k = P_k_list[best]
                    no_change = 0
                else:
                    no_change += 1
        # final mutual information
        return self.mutual_information, self.labels.copy()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="partycls.clustering.CommunityInference"><code class="flex name class">
<span>class <span class="ident">CommunityInference</span></span>
<span>(</span><span>n_clusters=2, n_init=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Community Inference is a hard clustering method based on information
theory. See "Paret et al. <a href="https://doi.org/10.1063/5.0004732&quot;">https://doi.org/10.1063/5.0004732"</a> for more
details.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CommunityInference(Clustering):
    &#34;&#34;&#34;
    Community Inference is a hard clustering method based on information 
    theory. See &#34;Paret et al. https://doi.org/10.1063/5.0004732&#34; for more 
    details.
    &#34;&#34;&#34;

    def __init__(self, n_clusters=2, n_init=1):
        self.symbol = &#39;cinf&#39;
        self.full_name = &#39;Community Inference&#39;
        Clustering.__init__(self, n_clusters=n_clusters, n_init=n_init)
        self.mutual_information = None

    def fit(self, X):
        &#34;&#34;&#34;
        Community inference algorithm.
        &#34;&#34;&#34;
        if isinstance(X, StructuralDescriptor):
            descriptor = X
        else:
            descriptor = DummyDescriptor()
            features = numpy.empty_like(X)
            bins = X.shape[1]
            for n in range(features.shape[0]):
                features[n] = numpy.histogram(X[n], bins=bins)[0]
            descriptor.features = features

        MI_previous, labels_previous = self._inference_loop(descriptor)
        for n in range(self.n_init - 1):
            MI_next, labels_next = self._inference_loop(descriptor)
            # optimization `n` is worse than the previous one
            if MI_next &lt; MI_previous:
                self.mutual_information = MI_previous
                self.labels = labels_previous
            # optimization `n` is better than the previous one
            # it becomes the new standard
            else:
                MI_previous = MI_next
                labels_previous = labels_next

    def _inference_loop(self, descriptor):

        import random

        # shortcuts
        N = descriptor.size
        K = self.n_clusters
        Km1 = K - 1  # loop invariant

        # randomly set the labels
        self.labels = numpy.array([random.randint(0, Km1) for n in range(N)])
        # populations and fractions
        n_k = self.populations
        f_k = self.fractions
        # community histograms
        dtype = descriptor.features.dtype
        H_k = numpy.zeros((K, descriptor.n_features), dtype=dtype)
        for i in range(N):
            k_i = self.labels[i]
            H_k[k_i] += descriptor.features[i]
        # community distributions
        P_k = numpy.empty_like(H_k, dtype=numpy.float64)
        for k in range(K):
            P_k[k] = descriptor.normalize(H_k[k] / n_k[k])
        # average distribution
        P_average = f_k @ P_k

        # community information
        if isinstance(descriptor, BondOrientationalDescriptor):
            # in case of non-successive values of l in the grid
            dx = 1.0
        else:
            dx = descriptor.grid[1] - descriptor.grid[0]

        def _mutual_information(P_average, P_k, f_k, dx):
            MI = 0.0
            f_x = numpy.empty_like(P_average)
            for k in range(len(f_k)):
                for m in range(len(P_average)):
                    if P_k[k, m] != 0.0 and P_average[m] != 0.0:
                        f_x[m] = P_k[k, m] * numpy.log2(P_k[k, m] / P_average[m])
                    else:
                        f_x[m] = 0.0

#                #TODO: is it more efficient?
#                f_x = P_k[k] * (numpy.log2(P_k[k],
#                         out=numpy.zeros_like(P_average),
#                         where=(P_k[k] != 0.0))
#                         - numpy.log2(P_average,
#                         out=numpy.zeros_like(P_average),
#                         where=(P_average != 0.0)))

                f_x = f_x * f_k[k]
                MI = MI + numpy.sum(f_x) * dx
            return MI
        self.mutual_information = _mutual_information(P_average, P_k, f_k, dx)

        # Begin loop
        no_change = 0
        while no_change &lt; N:
            # sample particles
            sample = random.sample(range(N), N)
            for i in sample:
                k_i = self.labels[i]
                k_others = [k for k in range(K) if k != k_i]
                # lists to store attemps for relocation of `i`
                labels_list = [numpy.empty_like(self.labels) for k in range(Km1)]
                n_k_list = [numpy.empty_like(n_k) for k in range(Km1)]
                H_k_list = [numpy.empty_like(H_k) for k in range(Km1)]
                P_k_list = [numpy.empty_like(P_k) for k in range(Km1)]
                I_list = [0.0 for k in range(Km1)]
                # loop over all other available communnities
                for kn, k_j in enumerate(k_others):
                    labels_new = self.labels.copy()
                    n_k_new = n_k.copy()
                    H_k_new = H_k.copy()
                    P_k_new = P_k.copy()
                    # k_i --&gt; k_j
                    labels_new[i] = k_j
                    labels_list[kn] = labels_new
                    # changes in populations and fractions
                    n_k_new[k_i] -= 1
                    n_k_new[k_j] += 1
                    n_k_list[kn] = n_k_new
                    f_k_new = n_k_new / N
                    # changes in histograms
                    H_k_new[k_i] -= descriptor.features[i]
                    H_k_new[k_j] += descriptor.features[i]
                    H_k_list[kn] = H_k_new
                    # changes in distributions
                    P_k_new[k_i] = descriptor.normalize(H_k_new[k_i] / n_k_new[k_i])
                    P_k_new[k_j] = descriptor.normalize(H_k_new[k_j] / n_k_new[k_j])
                    P_k_list[kn] = P_k_new
                    # change in community information
                    I_new = _mutual_information(P_average, P_k_new, f_k_new, dx)
                    I_list[kn] = I_new
                # find the most appropriate community for `i`
                best = numpy.argmax(I_list)
                if I_list[best] &gt; self.mutual_information:
                    self.labels = labels_list[best]
                    self.mutual_information = I_list[best]
                    n_k = n_k_list[best]
                    H_k = H_k_list[best]
                    P_k = P_k_list[best]
                    no_change = 0
                else:
                    no_change += 1
        # final mutual information
        return self.mutual_information, self.labels.copy()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>partycls.clustering.Clustering</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="partycls.clustering.CommunityInference.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Community inference algorithm.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X):
    &#34;&#34;&#34;
    Community inference algorithm.
    &#34;&#34;&#34;
    if isinstance(X, StructuralDescriptor):
        descriptor = X
    else:
        descriptor = DummyDescriptor()
        features = numpy.empty_like(X)
        bins = X.shape[1]
        for n in range(features.shape[0]):
            features[n] = numpy.histogram(X[n], bins=bins)[0]
        descriptor.features = features

    MI_previous, labels_previous = self._inference_loop(descriptor)
    for n in range(self.n_init - 1):
        MI_next, labels_next = self._inference_loop(descriptor)
        # optimization `n` is worse than the previous one
        if MI_next &lt; MI_previous:
            self.mutual_information = MI_previous
            self.labels = labels_previous
        # optimization `n` is better than the previous one
        # it becomes the new standard
        else:
            MI_previous = MI_next
            labels_previous = labels_next</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="partycls.clustering.GaussianMixture"><code class="flex name class">
<span>class <span class="ident">GaussianMixture</span></span>
<span>(</span><span>n_clusters=2, n_init=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Gaussian Mixture.</p>
<p>This class relies on the class <code><a title="partycls.clustering.GaussianMixture" href="#partycls.clustering.GaussianMixture">GaussianMixture</a></code> from the machine learning
package "scikit-learn". An instance of sklearn.mixture.GaussianMixture is
created when calling the <code>fit</code> method, and is then accessible through the
<code>backend</code> attribute for later use. See scikit's documentation for more
information on the original class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GaussianMixture(Clustering):
    &#34;&#34;&#34;
    Gaussian Mixture.
    
    This class relies on the class `GaussianMixture` from the machine learning 
    package &#34;scikit-learn&#34;. An instance of sklearn.mixture.GaussianMixture is 
    created when calling the `fit` method, and is then accessible through the 
    `backend` attribute for later use. See scikit&#39;s documentation for more 
    information on the original class.
    &#34;&#34;&#34;

    def __init__(self, n_clusters=2, n_init=1):
        self.symbol = &#39;gmm&#39;
        self.full_name = &#39;Gaussian Mixture&#39;
        Clustering.__init__(self, n_clusters=n_clusters, n_init=n_init)

    def fit(self, X):
        &#34;&#34;&#34;
        Run the EM algorithm on `X` using a mixture of Gaussians.
        The predicted labels are updated in the attribute `labels` of the 
        current instance of `GaussianMixture`.
        &#34;&#34;&#34;
        self.backend = _GaussianMixture(n_components=self.n_clusters,
                                        n_init=self.n_init)
        if hasattr(X, &#39;features&#39;):
            self.backend.fit(X.features)
        else:
            self.backend.fit(X)
        self.labels = self.backend.predict(X)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>partycls.clustering.Clustering</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="partycls.clustering.GaussianMixture.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Run the EM algorithm on <code>X</code> using a mixture of Gaussians.
The predicted labels are updated in the attribute <code>labels</code> of the
current instance of <code><a title="partycls.clustering.GaussianMixture" href="#partycls.clustering.GaussianMixture">GaussianMixture</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X):
    &#34;&#34;&#34;
    Run the EM algorithm on `X` using a mixture of Gaussians.
    The predicted labels are updated in the attribute `labels` of the 
    current instance of `GaussianMixture`.
    &#34;&#34;&#34;
    self.backend = _GaussianMixture(n_components=self.n_clusters,
                                    n_init=self.n_init)
    if hasattr(X, &#39;features&#39;):
        self.backend.fit(X.features)
    else:
        self.backend.fit(X)
    self.labels = self.backend.predict(X)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="partycls.clustering.KMeans"><code class="flex name class">
<span>class <span class="ident">KMeans</span></span>
<span>(</span><span>n_clusters=2, n_init=1)</span>
</code></dt>
<dd>
<div class="desc"><p>KMeans clustering.</p>
<p>This class relies on the class <code><a title="partycls.clustering.KMeans" href="#partycls.clustering.KMeans">KMeans</a></code> from the machine learning package
"scikit-learn". An instance of sklearn.cluster.KMeans is created when
calling the <code>fit</code> method, and is then accessible through the <code>backend</code>
attribute for later use. See scikit's documentation for more information on
the original class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class KMeans(Clustering):
    &#34;&#34;&#34;
    KMeans clustering.
    
    This class relies on the class `KMeans` from the machine learning package 
    &#34;scikit-learn&#34;. An instance of sklearn.cluster.KMeans is created when 
    calling the `fit` method, and is then accessible through the `backend`
    attribute for later use. See scikit&#39;s documentation for more information on
    the original class.
    &#34;&#34;&#34;

    def __init__(self, n_clusters=2, n_init=1):
        self.symbol = &#39;kmeans&#39;
        self.full_name = &#39;K-Means&#39;
        Clustering.__init__(self, n_clusters=n_clusters, n_init=n_init)

    def fit(self, X):
        &#34;&#34;&#34;
        Run the K-Means algorithm on `X`.
        The predicted labels are updated in the attribute `labels` of 
        the current instance of `KMeans`.
        &#34;&#34;&#34;
        self.backend = _KMeans(n_clusters=self.n_clusters,
                               n_init=self.n_init)
        if hasattr(X, &#39;features&#39;):
            self.backend.fit(X.features)
        else:
            self.backend.fit(X)
        self.labels = self.backend.labels_</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>partycls.clustering.Clustering</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="partycls.clustering.KMeans.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Run the K-Means algorithm on <code>X</code>.
The predicted labels are updated in the attribute <code>labels</code> of
the current instance of <code><a title="partycls.clustering.KMeans" href="#partycls.clustering.KMeans">KMeans</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X):
    &#34;&#34;&#34;
    Run the K-Means algorithm on `X`.
    The predicted labels are updated in the attribute `labels` of 
    the current instance of `KMeans`.
    &#34;&#34;&#34;
    self.backend = _KMeans(n_clusters=self.n_clusters,
                           n_init=self.n_init)
    if hasattr(X, &#39;features&#39;):
        self.backend.fit(X.features)
    else:
        self.backend.fit(X)
    self.labels = self.backend.labels_</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="partycls" href="index.html">partycls</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="partycls.clustering.CommunityInference" href="#partycls.clustering.CommunityInference">CommunityInference</a></code></h4>
<ul class="">
<li><code><a title="partycls.clustering.CommunityInference.fit" href="#partycls.clustering.CommunityInference.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="partycls.clustering.GaussianMixture" href="#partycls.clustering.GaussianMixture">GaussianMixture</a></code></h4>
<ul class="">
<li><code><a title="partycls.clustering.GaussianMixture.fit" href="#partycls.clustering.GaussianMixture.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="partycls.clustering.KMeans" href="#partycls.clustering.KMeans">KMeans</a></code></h4>
<ul class="">
<li><code><a title="partycls.clustering.KMeans.fit" href="#partycls.clustering.KMeans.fit">fit</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>