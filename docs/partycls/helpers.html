<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>partycls.helpers API documentation</title>
<meta name="description" content="Various helper functions for visualization, cluster analysis, etc." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>partycls.helpers</code></h1>
</header>
<section id="section-intro">
<p>Various helper functions for visualization, cluster analysis, etc.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Various helper functions for visualization, cluster analysis, etc.
&#34;&#34;&#34;

import numpy
from sklearn.metrics import adjusted_mutual_info_score as AMI
from sklearn.metrics import adjusted_rand_score as ARI
from sklearn.metrics import silhouette_samples, silhouette_score

__all__ = [&#39;AMI&#39;,
           &#39;ARI&#39;,
           &#39;silhouette_samples&#39;,
           &#39;silhouette_score&#39;,
           &#39;show_matplotlib&#39;,
           &#39;show_ovito&#39;,
           &#39;show_3dmol&#39;,
           &#39;shannon_entropy&#39;,
           &#39;merge_clusters&#39;,
           &#39;sort_clusters&#39;]

_palette = [&#34;#50514f&#34;, &#34;#f25f5c&#34;, &#34;#ffe066&#34;, &#34;#247ba0&#34;, &#34;#70c1b3&#34;,
            &#34;#0cce6b&#34;, &#34;#c200fb&#34;, &#34;#e2a0ff&#34;, &#34;#6622cc&#34;, &#34;#119822&#34;]

def hex_to_rgb(h):
    h = h.lstrip(&#39;#&#39;)
    return tuple(int(h[i:i+2], 16) / 256 for i in (0, 2, 4))


def show_matplotlib(system, color, view=&#39;top&#39;, palette=None, cmap=&#39;viridis&#39;,
                    outfile=None, linewidth=0.5, alpha=1.0, show=False):
    &#34;&#34;&#34;
    Make a snapshot of the `system` using matplotlib.
    The figure is returned for further customization or visualization 
    in jupyter notebooks.    

    Parameters
    ----------
    system : System
        An instance of `System`.
    color : str
        Particle property to use for color coding, e.g. &#39;species&#39;, &#39;label&#39;.
    view : str, optional
        View type, i.e. face of the box to show. Only works for a 3D system.
        The default is &#39;top&#39;.
    palette : list, optional
        List of colors when coloring particles according to a discrete property,
        such as &#39;species&#39; or &#39;label&#39;. A default palette will be used if not 
        specified. The default is None.
    cmap : str, optional
        Name of a matplotlib colormap to use when coloring particles according
        to a continuous property such as &#39;velocity&#39; or &#39;energy&#39;. List of 
        available colormap can be found in `matplotlib.cm.cmaps_listed`.
        The default is &#39;viridis&#39;.
    outfile : str, optional
        Output filename to save the snapshot. The default is None (not saved).
    linewidth : int or float, optional
        The default is 0.5.
    alpha : int or float, optional
        Transparency parameter. The default is 1.0.
    show : bool, optional
        Show the snapshot when calling the function. The default is False.

    Returns
    -------
    fig : matplotlib.figure.Figure
        Figure of the snapshot.

    &#34;&#34;&#34;
    import matplotlib.pyplot as plt
    from .core.utils import tipify
    from matplotlib.cm import cmaps_listed
    from numpy import array, sign, argsort

    views = {&#39;top&#39;: [1, 2, 3],
             &#39;bottom&#39;: [1, -2, -3],
             &#39;front&#39;: [1, 3, -2],
             &#39;back&#39;: [-1, 3, 2],
             &#39;left&#39;: [-2, 3, -1],
             &#39;right&#39;: [2, 3, 1]}

    fig = plt.figure()
    ax = fig.add_subplot(111, aspect=&#39;equal&#39;)
    ax.axes.get_xaxis().set_visible(False)
    ax.axes.get_yaxis().set_visible(False)
    ax.set_xlim((-system.cell.side[0] / 2, system.cell.side[0] / 2))
    ax.set_ylim((-system.cell.side[1] / 2, system.cell.side[1] / 2))
    # scale marker size relative to box size
    M = ax.transData.get_matrix()
    scale = M[0, 0]
    # color according to a specific property
    property_vals = system.get_property(&#39;particle.{}&#39;.format(color))

    # discrete property?
    discrete = isinstance(tipify(str(property_vals[0])), (str, int))
    # corresponding color system
    discrete_colors = _palette if palette is None else palette
    colormap = cmaps_listed[cmap]
    if discrete:
        property_set = list(set(property_vals))
        property_set.sort()
        color_db = discrete_colors
    else:
        color_db = colormap(property_vals)

    # list of individual colors
    colors = []
    for pn, p in enumerate(system.particle):
        if discrete:
            p_color = color_db[property_set.index(p.__getattribute__(color))]
            colors.append(p_color)
        else:
            colors.append(color_db[pn])
    colors = array(colors)

    # positions and radii
    pos = system.get_property(&#39;position&#39;)
    R = system.get_property(&#39;radius&#39;)

    # plot 3D
    if system.n_dimensions == 3:
        xi, yi, zi = views[view]
        X = sign(xi) * pos[:, abs(xi) - 1]
        Y = sign(yi) * pos[:, abs(yi) - 1]
        Z = sign(zi) * pos[:, abs(zi) - 1]
        order = argsort(Z)
        ax.scatter(X[order], Y[order], c=colors[order],
                   marker=&#39;o&#39;, ec=&#39;k&#39;, s=(scale * R[order])**2,
                   linewidths=linewidth, alpha=alpha)
    # plot 2D
    if system.n_dimensions == 2:
        X = pos[:, 0]
        Y = pos[:, 1]
        ax.scatter(X, Y, c=colors, marker=&#39;o&#39;, ec=&#39;k&#39;, s=(scale * R)**2,
                   linewidths=linewidth, alpha=alpha)

    if outfile is not None:
        fig.savefig(outfile, bbox_inches=&#39;tight&#39;)
    if show:
        plt.show()
    return fig


def show_ovito(system, color, view=&#39;top&#39;, palette=None, cmap=&#39;viridis&#39;,
               outfile=None, size=(640, 480), zoom=True):
    &#34;&#34;&#34;
    Make a snapshot of the `system` using Ovito.
    The image is returned for further customization or visualization 
    in jupyter notebooks.        

    Parameters
    ----------
    system : System
        An instance of `System`.
    color : str
        Particle property to use for color coding, e.g. &#39;species&#39;, &#39;label&#39;.
    view : str, optional
        View type, i.e. face of the box to show. Only works for a 3D system.
        The default is &#39;top&#39;.
    palette : list, optional
        List of colors when coloring particles according to a discrete property,
        such as &#39;species&#39; or &#39;label&#39;. A default palette will be used if not 
        specified. The default is None.
    cmap : str, optional
        Name of a matplotlib colormap to use when coloring particles according
        to a continuous property such as &#39;velocity&#39; or &#39;energy&#39;. List of 
        available colormap can be found in `matplotlib.cm.cmaps_listed`.
        The default is &#39;viridis&#39;.
    outfile : str, optional
        Output filename to save the snapshot. The default is None (not saved).
    size : tuple, optional
        Size of the image to render. The default is (640, 480).
    zoom : bool, optional
        Zoom on the simulation box. The default is True.

    Returns
    -------
    Image
        Rendered image.

    &#34;&#34;&#34;
    try:
        from ovito.io import import_file
    except ImportError:
        print(&#39;install ovito to display the particles&#39;)
        return
    from ovito.vis import Viewport, TachyonRenderer
    import os
    import tempfile
    from .core.utils import tipify
    from matplotlib.cm import cmaps_listed

    # discrete property?
    property_vals = system.get_property(&#39;particle.{}&#39;.format(color))
    discrete = isinstance(tipify(str(property_vals[0])), (str, int))
    # corresponding color system
    discrete_colors = _palette if palette is None else palette
    colormap = cmaps_listed[cmap]

    # discrete or continuous color field
    if palette is None:
        discrete_colors = _palette
        discrete_colors = [hex_to_rgb(c) for c in discrete_colors]
    else:
        discrete_colors = palette
    # corresponding palette / colormap
    if discrete:
        property_set = list(set(property_vals))
        property_set.sort()
        color_db = discrete_colors
    else:
        color_db = colormap(property_vals)
    
    # individual particle color
    for pn, p in enumerate(system.particle):
        if discrete:
            p_color = color_db[property_set.index(p.__getattribute__(color))]
            p.color = p_color
        else:
            p.color = color_db[pn]
    
    # Get a temporary file to write the sample
    fh = tempfile.NamedTemporaryFile(&#39;w&#39;, suffix=&#39;.xyz&#39;, delete=False)
    tmp_file = fh.name
    # Self-contained EXYZ dump (it is not clean to use trajectories here)
    fh.write(&#39;{}\n&#39;.format(len(system.particle)))
    fh.write(&#39;Properties=species:S:1:pos:R:3:radius:R:1:color:R:3 Lattice=&#34;{},0.,0.,0.,{},0.,0.,0.,{}&#34;\n&#39;.format(*system.cell.side))
    for p in system.particle:
        fh.write(&#39;{} {} {} {} {} {} {} {}\n&#39;.format(p.species, *p.position, p.radius, *p.color))
    fh.close()

    # Ovito stuff. Can be customized by client code.
    pipeline = import_file(tmp_file)
    # Ovito seems to ignore the lattice info of exyz file
    # so we forcibly set the cell info here
    pipeline.source.data.cell_[0, 0] = system.cell.side[0]
    pipeline.source.data.cell_[1, 1] = system.cell.side[1]
    pipeline.source.data.cell_[2, 2] = system.cell.side[2]
    pipeline.source.data.cell_[:, 3] = -system.cell.side/2
    pipeline.add_to_scene()

    views = {&#39;top&#39;:    Viewport.Type.Top,
             &#39;bottom&#39;: Viewport.Type.Bottom,
             &#39;front&#39;:  Viewport.Type.Front,
             &#39;back&#39;:   Viewport.Type.Back,
             &#39;left&#39;:   Viewport.Type.Left,
             &#39;right&#39;:  Viewport.Type.Right}
    vp_type = views[view]
    vp = Viewport(type=vp_type)

    # Render
    if zoom:
        vp.zoom_all()
    if outfile is None:
        outfile = tmp_file + &#39;.png&#39;
        
    vp.render_image(filename=outfile, 
                    size=size, 
                    renderer=TachyonRenderer())

    # Scene is a singleton, so we must clear it
    pipeline.remove_from_scene()
    
    # remove temporary exyz file
    os.remove(tmp_file)

    # Try to display the image (e.g. in a jupyter notebook)
    try:
        from IPython.display import Image
        return Image(outfile)
    except ImportError:
        return outfile


def show_3dmol(system, color, palette=None):
    &#34;&#34;&#34;
    Visualize the `system` using 3dmol http://3dmol.csb.pitt.edu/
    The py3Dmol view is returned for further customization or visualization 
    in jupyter notebooks.

    Parameters
    ----------
    system : System
        An instance of `System`.
    color : str
        Particle property to use for color coding, e.g. &#39;species&#39;, &#39;label&#39;.
        This property must be a string or an integer.
    palette : list, optional
        List of colors when coloring particles according to a discrete property,
        such as &#39;species&#39; or &#39;label&#39;. A default palette will be used if not 
        specified. The default is None.

    Raises
    ------
    ValueError
        If the `color` parameter refers to a float particle property.

    Returns
    -------
    view : py3Dmol.view
        py3Dmol view.

    &#34;&#34;&#34;
    import py3Dmol
    from .trajectory import tipify

    if palette is None:
        palette = _palette
    view = py3Dmol.view()
    view.setBackgroundColor(&#39;white&#39;)
    # color according to a specific property
    property_vals = system.get_property(&#39;particle.{}&#39;.format(color))
    property_set = list(set(property_vals))
    property_set.sort()
    if not isinstance(tipify(str(property_vals[0])), (str, int)):
        raise ValueError(&#39;cannot color particle according to a float property&#39;)
    # plot particles
    for p in system.particle:
        p_color = palette[property_set.index(p.__getattribute__(color))]
        view.addSphere({&#39;center&#39;: {&#39;x&#39;: p.position[0],
                                   &#39;y&#39;: p.position[1],
                                   &#39;z&#39;: p.position[2]},
                        &#39;radius&#39;: p.radius,
                        &#39;color&#39;: p_color})
    # plot cell
    view.addBox({&#39;center&#39;: {&#39;x&#39;: 0.0,
                            &#39;y&#39;: 0.0,
                            &#39;z&#39;: 0.0},
                 &#39;dimensions&#39;: {&#39;w&#39;: system.cell.side[0],
                                &#39;h&#39;: system.cell.side[1],
                                &#39;d&#39;: system.cell.side[2]},
                 &#39;wireframe&#39;: True, &#39;color&#39;: &#34;#000000&#34;})
    return view


def shannon_entropy(px, dx=1.0):
    &#34;&#34;&#34;
    Shannon entropy of distribution p(x).

    Parameters
    ----------
    px : list or numpy.array
        Distribution p(x).
    dx : float, optional
        Differential of x. The default is 1.0.

    Returns
    -------
    S : float
        Shannon entropy.
    &#34;&#34;&#34;
    S = 0.0
    P = px * dx
    for p in P:
        if p != 0.0:
            S += p * numpy.log(p)
    return -S


def merge_clusters(weights, n_clusters_min=2, epsilon_=1e-15):
    &#34;&#34;&#34;
    Merge clusters into `n_clusters_min` new clusters based on the
    probabilities that particles initially belong to each of the original
    clusters with a certain probability and using an entropy criterion.
    
    See https://doi.org/10.1198/jcgs.2010.08111 (Baudry et al.)

    Parameters
    ----------
    weights : list or numpy.ndarray
        Probabilities that each particle belongs to each cluster.
        If there are N particles, then the length of the list (or first
        dimension of the array) must be N. If there are K original clusters,
        each element of `weights` (or the first dimension of the array) must
        be K. `weights[i][j]` (list) or `weights[i,k]` (array) is the 
        probability that particle `i` belongs to cluster `k` before merging.
        For each particle, sum(weights[i]) = 1.
    n_clusters_min : int, optional
        Final number of clusters after merging. The default is 2.
    epsilon_ : float
        Small number (close to zero). This is needed as a replacement for zero
        when computing a logarithm to avoid errors. The default is 1e-15.

    Returns
    -------
    new_weights : numpy.ndarray
        New weights after merging. Same shape and interpretation as the
        `weights` input parameter.
    new_labels : list
        New discrete labels based on the weights after merging.
    &#34;&#34;&#34;
    new_weights = numpy.asarray(weights).copy()

    # number of clusters (to be changed during the merge)
    n_clusters = weights[0].size

    # print(&#34;# i  j  delta_entropy&#34;)
    while n_clusters &gt; n_clusters_min:
        # loop over all pairs of clusters and consider what happens if we merge
        d_evals = []
        labs = []
        for i in range(new_weights[0].size):  # i loops over clusters
            for j in range(i):
                delta_ent = _compute_delta_ent(i, j, new_weights)
                d_evals.append(delta_ent)
                labs.append([i, j])
                # print entropy change for (i,j)
                # print(&#39;  {i}  {j}  {:.4f}&#39;.format(i,j,delta_ent))

        best_merge_index = d_evals.index(max(d_evals))
        # print(&#34;# best merge: {}, gain={:.4f}&#34;.format(labs[best_merge_index],
        #                                              d_evals[best_merge_index]))

        # do the merge...
        for w in new_weights:
            w[labs[best_merge_index][1]] += w[labs[best_merge_index][0]]
            w[labs[best_merge_index][0]] = epsilon_

        # print(&#34;# ICL entropy before (total) : {:.4f}&#34;.format(_compute_ICL_ent(weights, epsilon_)))
        # print(&#34;# ICL entropy after  (total) : {:.4f}&#34;.format(_compute_ICL_ent(new_weights, epsilon_)))

        n_clusters -= 1

    # new discrete labels based on the new weights
    new_labels = [0 for n in range(new_weights.shape[0])]
    for i, wi in enumerate(new_weights):
        new_labels[i] = numpy.argmax(wi)

    return new_weights, new_labels


def sort_clusters(labels, centroids, func=shannon_entropy):
    &#34;&#34;&#34;
    Make a consistent labeling of the clusters based on their centroids by
    computing an associated numerical value as sorting criterion. By default, 
    the labeling is based on the Shannon entropy of each cluster.

    Parameters
    ----------
    labels : list
        Original labels.
    centroids : numpy.ndarray
        Cluster centroids.
    func : function, optional
        Function used to associate a numerical value to each cluster, to be
        used as sorting criterion. This function must accept a list or a 
        one dimensional array as parameter (this parameter being the 
        coordinates of a given centroid). The default is shannon_entropy.

    Returns
    -------
    new_labels : list
        New labels based on centroid entropies.
    new_centroids : numpy.ndarray
        Centroids arranged in order of descending entropies.

    &#34;&#34;&#34;
    n_clusters = centroids.shape[0]
    new_centroids = numpy.zeros_like(centroids)
    new_labels = labels.copy()
    entropies = numpy.empty(n_clusters)
    # Shannon entropy of each cluster based on centroids
    for k in range(n_clusters):
        entropies[k] = func(centroids[k])
    # rank distributions according to their entropies
    entropies *= -1
    ranks = numpy.argsort(numpy.argsort(entropies))
    # Change particles
    for i in range(len(labels)):
        k_i = labels[i]
        k_new = ranks[k_i]
        new_labels[i] = k_new
    # change fractions, hist and dist
    for k in range(n_clusters):
        k_new = ranks[k]
        new_centroids[k_new] = centroids[k]
    return new_labels, new_centroids


def _compute_delta_ent(i, j, weights):
    &#34;&#34;&#34;
    Entropy change on merging two clusters (following Baudry) 
    &#34;&#34;&#34;
    delta_ent = 0.0
    for w in weights:  # each w is a vector of weights (this is a loop over particles)
        w_merge = w[i] + w[j]  # for this particle, add the weights for cluster i and j
        delta_ent += w_merge * numpy.log(w_merge)
        delta_ent -= w[i] * numpy.log(w[i])
        delta_ent -= w[j] * numpy.log(w[j])
    return delta_ent


def _compute_ICL_ent(weights, epsilon_):
    &#34;&#34;&#34;
    ICL entropy from list of weights
    &#34;&#34;&#34;
    ICL_ent = 0.0
    for w in weights:
        wts = numpy.maximum(w, epsilon_)
        ICL_ent -= numpy.sum(wts * numpy.log(wts))
    return ICL_ent</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="partycls.helpers.AMI"><code class="name flex">
<span>def <span class="ident">AMI</span></span>(<span>labels_true, labels_pred, *, average_method='arithmetic')</span>
</code></dt>
<dd>
<div class="desc"><p>Adjusted Mutual Information between two clusterings.</p>
<p>Adjusted Mutual Information (AMI) is an adjustment of the Mutual
Information (MI) score to account for chance. It accounts for the fact that
the MI is generally higher for two clusterings with a larger number of
clusters, regardless of whether there is actually more information shared.
For two clusterings :math:<code>U</code> and :math:<code>V</code>, the AMI is given as::</p>
<pre><code>AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]
</code></pre>
<p>This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won't change the
score value in any way.</p>
<p>This metric is furthermore symmetric: switching <code>label_true</code> with
<code>label_pred</code> will return the same score value. This can be useful to
measure the agreement of two independent label assignments strategies
on the same dataset when the real ground truth is not known.</p>
<p>Be mindful that this function is an order of magnitude slower than other
metrics, such as the Adjusted Rand Index.</p>
<p>Read more in the :ref:<code>User Guide &lt;mutual_info_score&gt;</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>labels_true</code></strong> :&ensp;<code>int array, shape = [n_samples]</code></dt>
<dd>A clustering of the data into disjoint subsets.</dd>
<dt><strong><code>labels_pred</code></strong> :&ensp;<code>int array-like</code> of <code>shape (n_samples,)</code></dt>
<dd>A clustering of the data into disjoint subsets.</dd>
<dt><strong><code>average_method</code></strong> :&ensp;<code>str</code>, default=<code>'arithmetic'</code></dt>
<dd>
<p>How to compute the normalizer in the denominator. Possible options
are 'min', 'geometric', 'arithmetic', and 'max'.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.20</p>
</div>
<div class="admonition versionchanged">
<p class="admonition-title">Changed in version:&ensp;0.22</p>
<p>The default value of <code>average_method</code> changed from 'max' to
'arithmetic'.</p>
</div>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ami</code></strong> :&ensp;<code>float (upperlimited by 1.0)</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>The AMI returns a value of 1 when the two partitions are identical
(ie perfectly matched). Random partitions (independent labellings) have
an expected AMI around 0 on average hence can be negative.</p>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>adjusted_rand_score</code></dt>
<dd>Adjusted Rand Index.</dd>
<dt><code>mutual_info_score</code></dt>
<dd>Mutual Information (not adjusted for chance).</dd>
</dl>
<h2 id="examples">Examples</h2>
<p>Perfect labelings are both homogeneous and complete, hence have
score 1.0::</p>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.metrics.cluster import adjusted_mutual_info_score
adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
&hellip; # doctest: +SKIP
1.0
adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
&hellip; # doctest: +SKIP
1.0</p>
</blockquote>
</blockquote>
</blockquote>
<p>If classes members are completely split across different clusters,
the assignment is totally in-complete, hence the AMI is null::</p>
<blockquote>
<blockquote>
<blockquote>
<p>adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
&hellip; # doctest: +SKIP
0.0</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="references">References</h2>
<p>.. [1] <code>Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for
Clusterings Comparison: Variants, Properties, Normalization and
Correction for Chance, JMLR
&lt;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&gt;</code>_</p>
<p>.. [2] <code>Wikipedia entry for the Adjusted Mutual Information
&lt;https://en.wikipedia.org/wiki/Adjusted_Mutual_Information&gt;</code>_</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_deprecate_positional_args
def adjusted_mutual_info_score(labels_true, labels_pred, *,
                               average_method=&#39;arithmetic&#39;):
    &#34;&#34;&#34;Adjusted Mutual Information between two clusterings.

    Adjusted Mutual Information (AMI) is an adjustment of the Mutual
    Information (MI) score to account for chance. It accounts for the fact that
    the MI is generally higher for two clusterings with a larger number of
    clusters, regardless of whether there is actually more information shared.
    For two clusterings :math:`U` and :math:`V`, the AMI is given as::

        AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]

    This metric is independent of the absolute values of the labels:
    a permutation of the class or cluster label values won&#39;t change the
    score value in any way.

    This metric is furthermore symmetric: switching ``label_true`` with
    ``label_pred`` will return the same score value. This can be useful to
    measure the agreement of two independent label assignments strategies
    on the same dataset when the real ground truth is not known.

    Be mindful that this function is an order of magnitude slower than other
    metrics, such as the Adjusted Rand Index.

    Read more in the :ref:`User Guide &lt;mutual_info_score&gt;`.

    Parameters
    ----------
    labels_true : int array, shape = [n_samples]
        A clustering of the data into disjoint subsets.

    labels_pred : int array-like of shape (n_samples,)
        A clustering of the data into disjoint subsets.

    average_method : str, default=&#39;arithmetic&#39;
        How to compute the normalizer in the denominator. Possible options
        are &#39;min&#39;, &#39;geometric&#39;, &#39;arithmetic&#39;, and &#39;max&#39;.

        .. versionadded:: 0.20

        .. versionchanged:: 0.22
           The default value of ``average_method`` changed from &#39;max&#39; to
           &#39;arithmetic&#39;.

    Returns
    -------
    ami: float (upperlimited by 1.0)
       The AMI returns a value of 1 when the two partitions are identical
       (ie perfectly matched). Random partitions (independent labellings) have
       an expected AMI around 0 on average hence can be negative.

    See Also
    --------
    adjusted_rand_score : Adjusted Rand Index.
    mutual_info_score : Mutual Information (not adjusted for chance).

    Examples
    --------

    Perfect labelings are both homogeneous and complete, hence have
    score 1.0::

      &gt;&gt;&gt; from sklearn.metrics.cluster import adjusted_mutual_info_score
      &gt;&gt;&gt; adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])
      ... # doctest: +SKIP
      1.0
      &gt;&gt;&gt; adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])
      ... # doctest: +SKIP
      1.0

    If classes members are completely split across different clusters,
    the assignment is totally in-complete, hence the AMI is null::

      &gt;&gt;&gt; adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])
      ... # doctest: +SKIP
      0.0

    References
    ----------
    .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for
       Clusterings Comparison: Variants, Properties, Normalization and
       Correction for Chance, JMLR
       &lt;http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf&gt;`_

    .. [2] `Wikipedia entry for the Adjusted Mutual Information
       &lt;https://en.wikipedia.org/wiki/Adjusted_Mutual_Information&gt;`_
    &#34;&#34;&#34;
    labels_true, labels_pred = check_clusterings(labels_true, labels_pred)
    n_samples = labels_true.shape[0]
    classes = np.unique(labels_true)
    clusters = np.unique(labels_pred)
    # Special limit cases: no clustering since the data is not split.
    # This is a perfect match hence return 1.0.
    if (classes.shape[0] == clusters.shape[0] == 1 or
            classes.shape[0] == clusters.shape[0] == 0):
        return 1.0
    contingency = contingency_matrix(labels_true, labels_pred, sparse=True)
    contingency = contingency.astype(np.float64,
                                     **_astype_copy_false(contingency))
    # Calculate the MI for the two clusterings
    mi = mutual_info_score(labels_true, labels_pred,
                           contingency=contingency)
    # Calculate the expected value for the mutual information
    emi = expected_mutual_information(contingency, n_samples)
    # Calculate entropy for each labeling
    h_true, h_pred = entropy(labels_true), entropy(labels_pred)
    normalizer = _generalized_average(h_true, h_pred, average_method)
    denominator = normalizer - emi
    # Avoid 0.0 / 0.0 when expectation equals maximum, i.e a perfect match.
    # normalizer should always be &gt;= emi, but because of floating-point
    # representation, sometimes emi is slightly larger. Correct this
    # by preserving the sign.
    if denominator &lt; 0:
        denominator = min(denominator, -np.finfo(&#39;float64&#39;).eps)
    else:
        denominator = max(denominator, np.finfo(&#39;float64&#39;).eps)
    ami = (mi - emi) / denominator
    return ami</code></pre>
</details>
</dd>
<dt id="partycls.helpers.ARI"><code class="name flex">
<span>def <span class="ident">ARI</span></span>(<span>labels_true, labels_pred)</span>
</code></dt>
<dd>
<div class="desc"><p>Rand index adjusted for chance.</p>
<p>The Rand Index computes a similarity measure between two clusterings
by considering all pairs of samples and counting pairs that are
assigned in the same or different clusters in the predicted and
true clusterings.</p>
<p>The raw RI score is then "adjusted for chance" into the ARI score
using the following scheme::</p>
<pre><code>ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)
</code></pre>
<p>The adjusted Rand index is thus ensured to have a value close to
0.0 for random labeling independently of the number of clusters and
samples and exactly 1.0 when the clusterings are identical (up to
a permutation).</p>
<p>ARI is a symmetric measure::</p>
<pre><code>adjusted_rand_score(a, b) == adjusted_rand_score(b, a)
</code></pre>
<p>Read more in the :ref:<code>User Guide &lt;adjusted_rand_score&gt;</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>labels_true</code></strong> :&ensp;<code>int array, shape = [n_samples]</code></dt>
<dd>Ground truth class labels to be used as a reference</dd>
<dt><strong><code>labels_pred</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples,)</code></dt>
<dd>Cluster labels to evaluate</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ARI</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Similarity score between -1.0 and 1.0. Random labelings have an ARI
close to 0.0. 1.0 stands for perfect match.</p>
<h2 id="examples">Examples</h2>
<p>Perfectly matching labelings have a score of 1 even</p>
<blockquote>
<blockquote>
<blockquote>
<p>from sklearn.metrics.cluster import adjusted_rand_score
adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])
1.0
adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])
1.0</p>
</blockquote>
</blockquote>
</blockquote>
<p>Labelings that assign all classes members to the same clusters
are complete but may not always be pure, hence penalized::</p>
<blockquote>
<blockquote>
<blockquote>
<p>adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])
0.57&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<p>ARI is symmetric, so labelings that have pure clusters with members
coming from the same classes but unnecessary splits are penalized::</p>
<blockquote>
<blockquote>
<blockquote>
<p>adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])
0.57&hellip;</p>
</blockquote>
</blockquote>
</blockquote>
<p>If classes members are completely split across different clusters, the
assignment is totally incomplete, hence the ARI is very low::</p>
<blockquote>
<blockquote>
<blockquote>
<p>adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])
0.0</p>
</blockquote>
</blockquote>
</blockquote>
<h2 id="references">References</h2>
<p>.. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,
Journal of Classification 1985
<a href="https://link.springer.com/article/10.1007%2FBF01908075">https://link.springer.com/article/10.1007%2FBF01908075</a></p>
<p>.. [Steinley2004] D. Steinley, Properties of the Hubert-Arabie
adjusted Rand index, Psychological Methods 2004</p>
<p>.. [wk] <a href="https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index">https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index</a></p>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>adjusted_mutual_info_score</code></dt>
<dd>Adjusted Mutual Information.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adjusted_rand_score(labels_true, labels_pred):
    &#34;&#34;&#34;Rand index adjusted for chance.

    The Rand Index computes a similarity measure between two clusterings
    by considering all pairs of samples and counting pairs that are
    assigned in the same or different clusters in the predicted and
    true clusterings.

    The raw RI score is then &#34;adjusted for chance&#34; into the ARI score
    using the following scheme::

        ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)

    The adjusted Rand index is thus ensured to have a value close to
    0.0 for random labeling independently of the number of clusters and
    samples and exactly 1.0 when the clusterings are identical (up to
    a permutation).

    ARI is a symmetric measure::

        adjusted_rand_score(a, b) == adjusted_rand_score(b, a)

    Read more in the :ref:`User Guide &lt;adjusted_rand_score&gt;`.

    Parameters
    ----------
    labels_true : int array, shape = [n_samples]
        Ground truth class labels to be used as a reference

    labels_pred : array-like of shape (n_samples,)
        Cluster labels to evaluate

    Returns
    -------
    ARI : float
       Similarity score between -1.0 and 1.0. Random labelings have an ARI
       close to 0.0. 1.0 stands for perfect match.

    Examples
    --------
    Perfectly matching labelings have a score of 1 even

      &gt;&gt;&gt; from sklearn.metrics.cluster import adjusted_rand_score
      &gt;&gt;&gt; adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])
      1.0
      &gt;&gt;&gt; adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])
      1.0

    Labelings that assign all classes members to the same clusters
    are complete but may not always be pure, hence penalized::

      &gt;&gt;&gt; adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])
      0.57...

    ARI is symmetric, so labelings that have pure clusters with members
    coming from the same classes but unnecessary splits are penalized::

      &gt;&gt;&gt; adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])
      0.57...

    If classes members are completely split across different clusters, the
    assignment is totally incomplete, hence the ARI is very low::

      &gt;&gt;&gt; adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])
      0.0

    References
    ----------
    .. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,
      Journal of Classification 1985
      https://link.springer.com/article/10.1007%2FBF01908075

    .. [Steinley2004] D. Steinley, Properties of the Hubert-Arabie
      adjusted Rand index, Psychological Methods 2004

    .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index

    See Also
    --------
    adjusted_mutual_info_score : Adjusted Mutual Information.
    &#34;&#34;&#34;
    (tn, fp), (fn, tp) = pair_confusion_matrix(labels_true, labels_pred)

    # Special cases: empty data or full agreement
    if fn == 0 and fp == 0:
        return 1.0

    return 2. * (tp * tn - fn * fp) / ((tp + fn) * (fn + tn) +
                                       (tp + fp) * (fp + tn))</code></pre>
</details>
</dd>
<dt id="partycls.helpers.merge_clusters"><code class="name flex">
<span>def <span class="ident">merge_clusters</span></span>(<span>weights, n_clusters_min=2, epsilon_=1e-15)</span>
</code></dt>
<dd>
<div class="desc"><p>Merge clusters into <code>n_clusters_min</code> new clusters based on the
probabilities that particles initially belong to each of the original
clusters with a certain probability and using an entropy criterion.</p>
<p>See <a href="https://doi.org/10.1198/jcgs.2010.08111">https://doi.org/10.1198/jcgs.2010.08111</a> (Baudry et al.)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>weights</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>Probabilities that each particle belongs to each cluster.
If there are N particles, then the length of the list (or first
dimension of the array) must be N. If there are K original clusters,
each element of <code>weights</code> (or the first dimension of the array) must
be K. <code>weights[i][j]</code> (list) or <code>weights[i,k]</code> (array) is the
probability that particle <code>i</code> belongs to cluster <code>k</code> before merging.
For each particle, sum(weights[i]) = 1.</dd>
<dt><strong><code>n_clusters_min</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Final number of clusters after merging. The default is 2.</dd>
<dt><strong><code>epsilon_</code></strong> :&ensp;<code>float</code></dt>
<dd>Small number (close to zero). This is needed as a replacement for zero
when computing a logarithm to avoid errors. The default is 1e-15.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>new_weights</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>New weights after merging. Same shape and interpretation as the
<code>weights</code> input parameter.</dd>
<dt><strong><code>new_labels</code></strong> :&ensp;<code>list</code></dt>
<dd>New discrete labels based on the weights after merging.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_clusters(weights, n_clusters_min=2, epsilon_=1e-15):
    &#34;&#34;&#34;
    Merge clusters into `n_clusters_min` new clusters based on the
    probabilities that particles initially belong to each of the original
    clusters with a certain probability and using an entropy criterion.
    
    See https://doi.org/10.1198/jcgs.2010.08111 (Baudry et al.)

    Parameters
    ----------
    weights : list or numpy.ndarray
        Probabilities that each particle belongs to each cluster.
        If there are N particles, then the length of the list (or first
        dimension of the array) must be N. If there are K original clusters,
        each element of `weights` (or the first dimension of the array) must
        be K. `weights[i][j]` (list) or `weights[i,k]` (array) is the 
        probability that particle `i` belongs to cluster `k` before merging.
        For each particle, sum(weights[i]) = 1.
    n_clusters_min : int, optional
        Final number of clusters after merging. The default is 2.
    epsilon_ : float
        Small number (close to zero). This is needed as a replacement for zero
        when computing a logarithm to avoid errors. The default is 1e-15.

    Returns
    -------
    new_weights : numpy.ndarray
        New weights after merging. Same shape and interpretation as the
        `weights` input parameter.
    new_labels : list
        New discrete labels based on the weights after merging.
    &#34;&#34;&#34;
    new_weights = numpy.asarray(weights).copy()

    # number of clusters (to be changed during the merge)
    n_clusters = weights[0].size

    # print(&#34;# i  j  delta_entropy&#34;)
    while n_clusters &gt; n_clusters_min:
        # loop over all pairs of clusters and consider what happens if we merge
        d_evals = []
        labs = []
        for i in range(new_weights[0].size):  # i loops over clusters
            for j in range(i):
                delta_ent = _compute_delta_ent(i, j, new_weights)
                d_evals.append(delta_ent)
                labs.append([i, j])
                # print entropy change for (i,j)
                # print(&#39;  {i}  {j}  {:.4f}&#39;.format(i,j,delta_ent))

        best_merge_index = d_evals.index(max(d_evals))
        # print(&#34;# best merge: {}, gain={:.4f}&#34;.format(labs[best_merge_index],
        #                                              d_evals[best_merge_index]))

        # do the merge...
        for w in new_weights:
            w[labs[best_merge_index][1]] += w[labs[best_merge_index][0]]
            w[labs[best_merge_index][0]] = epsilon_

        # print(&#34;# ICL entropy before (total) : {:.4f}&#34;.format(_compute_ICL_ent(weights, epsilon_)))
        # print(&#34;# ICL entropy after  (total) : {:.4f}&#34;.format(_compute_ICL_ent(new_weights, epsilon_)))

        n_clusters -= 1

    # new discrete labels based on the new weights
    new_labels = [0 for n in range(new_weights.shape[0])]
    for i, wi in enumerate(new_weights):
        new_labels[i] = numpy.argmax(wi)

    return new_weights, new_labels</code></pre>
</details>
</dd>
<dt id="partycls.helpers.shannon_entropy"><code class="name flex">
<span>def <span class="ident">shannon_entropy</span></span>(<span>px, dx=1.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Shannon entropy of distribution p(x).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>px</code></strong> :&ensp;<code>list</code> or <code>numpy.array</code></dt>
<dd>Distribution p(x).</dd>
<dt><strong><code>dx</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Differential of x. The default is 1.0.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>S</code></strong> :&ensp;<code>float</code></dt>
<dd>Shannon entropy.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shannon_entropy(px, dx=1.0):
    &#34;&#34;&#34;
    Shannon entropy of distribution p(x).

    Parameters
    ----------
    px : list or numpy.array
        Distribution p(x).
    dx : float, optional
        Differential of x. The default is 1.0.

    Returns
    -------
    S : float
        Shannon entropy.
    &#34;&#34;&#34;
    S = 0.0
    P = px * dx
    for p in P:
        if p != 0.0:
            S += p * numpy.log(p)
    return -S</code></pre>
</details>
</dd>
<dt id="partycls.helpers.show_3dmol"><code class="name flex">
<span>def <span class="ident">show_3dmol</span></span>(<span>system, color, palette=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Visualize the <code>system</code> using 3dmol <a href="http://3dmol.csb.pitt.edu/">http://3dmol.csb.pitt.edu/</a>
The py3Dmol view is returned for further customization or visualization
in jupyter notebooks.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>system</code></strong> :&ensp;<code>System</code></dt>
<dd>An instance of <code>System</code>.</dd>
<dt><strong><code>color</code></strong> :&ensp;<code>str</code></dt>
<dd>Particle property to use for color coding, e.g. 'species', 'label'.
This property must be a string or an integer.</dd>
<dt><strong><code>palette</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>List of colors when coloring particles according to a discrete property,
such as 'species' or 'label'. A default palette will be used if not
specified. The default is None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the <code>color</code> parameter refers to a float particle property.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>view</code></strong> :&ensp;<code>py3Dmol.view</code></dt>
<dd>py3Dmol view.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_3dmol(system, color, palette=None):
    &#34;&#34;&#34;
    Visualize the `system` using 3dmol http://3dmol.csb.pitt.edu/
    The py3Dmol view is returned for further customization or visualization 
    in jupyter notebooks.

    Parameters
    ----------
    system : System
        An instance of `System`.
    color : str
        Particle property to use for color coding, e.g. &#39;species&#39;, &#39;label&#39;.
        This property must be a string or an integer.
    palette : list, optional
        List of colors when coloring particles according to a discrete property,
        such as &#39;species&#39; or &#39;label&#39;. A default palette will be used if not 
        specified. The default is None.

    Raises
    ------
    ValueError
        If the `color` parameter refers to a float particle property.

    Returns
    -------
    view : py3Dmol.view
        py3Dmol view.

    &#34;&#34;&#34;
    import py3Dmol
    from .trajectory import tipify

    if palette is None:
        palette = _palette
    view = py3Dmol.view()
    view.setBackgroundColor(&#39;white&#39;)
    # color according to a specific property
    property_vals = system.get_property(&#39;particle.{}&#39;.format(color))
    property_set = list(set(property_vals))
    property_set.sort()
    if not isinstance(tipify(str(property_vals[0])), (str, int)):
        raise ValueError(&#39;cannot color particle according to a float property&#39;)
    # plot particles
    for p in system.particle:
        p_color = palette[property_set.index(p.__getattribute__(color))]
        view.addSphere({&#39;center&#39;: {&#39;x&#39;: p.position[0],
                                   &#39;y&#39;: p.position[1],
                                   &#39;z&#39;: p.position[2]},
                        &#39;radius&#39;: p.radius,
                        &#39;color&#39;: p_color})
    # plot cell
    view.addBox({&#39;center&#39;: {&#39;x&#39;: 0.0,
                            &#39;y&#39;: 0.0,
                            &#39;z&#39;: 0.0},
                 &#39;dimensions&#39;: {&#39;w&#39;: system.cell.side[0],
                                &#39;h&#39;: system.cell.side[1],
                                &#39;d&#39;: system.cell.side[2]},
                 &#39;wireframe&#39;: True, &#39;color&#39;: &#34;#000000&#34;})
    return view</code></pre>
</details>
</dd>
<dt id="partycls.helpers.show_matplotlib"><code class="name flex">
<span>def <span class="ident">show_matplotlib</span></span>(<span>system, color, view='top', palette=None, cmap='viridis', outfile=None, linewidth=0.5, alpha=1.0, show=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Make a snapshot of the <code>system</code> using matplotlib.
The figure is returned for further customization or visualization
in jupyter notebooks.
</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>system</code></strong> :&ensp;<code>System</code></dt>
<dd>An instance of <code>System</code>.</dd>
<dt><strong><code>color</code></strong> :&ensp;<code>str</code></dt>
<dd>Particle property to use for color coding, e.g. 'species', 'label'.</dd>
<dt><strong><code>view</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>View type, i.e. face of the box to show. Only works for a 3D system.
The default is 'top'.</dd>
<dt><strong><code>palette</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>List of colors when coloring particles according to a discrete property,
such as 'species' or 'label'. A default palette will be used if not
specified. The default is None.</dd>
<dt><strong><code>cmap</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of a matplotlib colormap to use when coloring particles according
to a continuous property such as 'velocity' or 'energy'. List of
available colormap can be found in <code>matplotlib.cm.cmaps_listed</code>.
The default is 'viridis'.</dd>
<dt><strong><code>outfile</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Output filename to save the snapshot. The default is None (not saved).</dd>
<dt><strong><code>linewidth</code></strong> :&ensp;<code>int</code> or <code>float</code>, optional</dt>
<dd>The default is 0.5.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>int</code> or <code>float</code>, optional</dt>
<dd>Transparency parameter. The default is 1.0.</dd>
<dt><strong><code>show</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Show the snapshot when calling the function. The default is False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>fig</code></strong> :&ensp;<code>matplotlib.figure.Figure</code></dt>
<dd>Figure of the snapshot.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_matplotlib(system, color, view=&#39;top&#39;, palette=None, cmap=&#39;viridis&#39;,
                    outfile=None, linewidth=0.5, alpha=1.0, show=False):
    &#34;&#34;&#34;
    Make a snapshot of the `system` using matplotlib.
    The figure is returned for further customization or visualization 
    in jupyter notebooks.    

    Parameters
    ----------
    system : System
        An instance of `System`.
    color : str
        Particle property to use for color coding, e.g. &#39;species&#39;, &#39;label&#39;.
    view : str, optional
        View type, i.e. face of the box to show. Only works for a 3D system.
        The default is &#39;top&#39;.
    palette : list, optional
        List of colors when coloring particles according to a discrete property,
        such as &#39;species&#39; or &#39;label&#39;. A default palette will be used if not 
        specified. The default is None.
    cmap : str, optional
        Name of a matplotlib colormap to use when coloring particles according
        to a continuous property such as &#39;velocity&#39; or &#39;energy&#39;. List of 
        available colormap can be found in `matplotlib.cm.cmaps_listed`.
        The default is &#39;viridis&#39;.
    outfile : str, optional
        Output filename to save the snapshot. The default is None (not saved).
    linewidth : int or float, optional
        The default is 0.5.
    alpha : int or float, optional
        Transparency parameter. The default is 1.0.
    show : bool, optional
        Show the snapshot when calling the function. The default is False.

    Returns
    -------
    fig : matplotlib.figure.Figure
        Figure of the snapshot.

    &#34;&#34;&#34;
    import matplotlib.pyplot as plt
    from .core.utils import tipify
    from matplotlib.cm import cmaps_listed
    from numpy import array, sign, argsort

    views = {&#39;top&#39;: [1, 2, 3],
             &#39;bottom&#39;: [1, -2, -3],
             &#39;front&#39;: [1, 3, -2],
             &#39;back&#39;: [-1, 3, 2],
             &#39;left&#39;: [-2, 3, -1],
             &#39;right&#39;: [2, 3, 1]}

    fig = plt.figure()
    ax = fig.add_subplot(111, aspect=&#39;equal&#39;)
    ax.axes.get_xaxis().set_visible(False)
    ax.axes.get_yaxis().set_visible(False)
    ax.set_xlim((-system.cell.side[0] / 2, system.cell.side[0] / 2))
    ax.set_ylim((-system.cell.side[1] / 2, system.cell.side[1] / 2))
    # scale marker size relative to box size
    M = ax.transData.get_matrix()
    scale = M[0, 0]
    # color according to a specific property
    property_vals = system.get_property(&#39;particle.{}&#39;.format(color))

    # discrete property?
    discrete = isinstance(tipify(str(property_vals[0])), (str, int))
    # corresponding color system
    discrete_colors = _palette if palette is None else palette
    colormap = cmaps_listed[cmap]
    if discrete:
        property_set = list(set(property_vals))
        property_set.sort()
        color_db = discrete_colors
    else:
        color_db = colormap(property_vals)

    # list of individual colors
    colors = []
    for pn, p in enumerate(system.particle):
        if discrete:
            p_color = color_db[property_set.index(p.__getattribute__(color))]
            colors.append(p_color)
        else:
            colors.append(color_db[pn])
    colors = array(colors)

    # positions and radii
    pos = system.get_property(&#39;position&#39;)
    R = system.get_property(&#39;radius&#39;)

    # plot 3D
    if system.n_dimensions == 3:
        xi, yi, zi = views[view]
        X = sign(xi) * pos[:, abs(xi) - 1]
        Y = sign(yi) * pos[:, abs(yi) - 1]
        Z = sign(zi) * pos[:, abs(zi) - 1]
        order = argsort(Z)
        ax.scatter(X[order], Y[order], c=colors[order],
                   marker=&#39;o&#39;, ec=&#39;k&#39;, s=(scale * R[order])**2,
                   linewidths=linewidth, alpha=alpha)
    # plot 2D
    if system.n_dimensions == 2:
        X = pos[:, 0]
        Y = pos[:, 1]
        ax.scatter(X, Y, c=colors, marker=&#39;o&#39;, ec=&#39;k&#39;, s=(scale * R)**2,
                   linewidths=linewidth, alpha=alpha)

    if outfile is not None:
        fig.savefig(outfile, bbox_inches=&#39;tight&#39;)
    if show:
        plt.show()
    return fig</code></pre>
</details>
</dd>
<dt id="partycls.helpers.show_ovito"><code class="name flex">
<span>def <span class="ident">show_ovito</span></span>(<span>system, color, view='top', palette=None, cmap='viridis', outfile=None, size=(640, 480), zoom=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Make a snapshot of the <code>system</code> using Ovito.
The image is returned for further customization or visualization
in jupyter notebooks.
</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>system</code></strong> :&ensp;<code>System</code></dt>
<dd>An instance of <code>System</code>.</dd>
<dt><strong><code>color</code></strong> :&ensp;<code>str</code></dt>
<dd>Particle property to use for color coding, e.g. 'species', 'label'.</dd>
<dt><strong><code>view</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>View type, i.e. face of the box to show. Only works for a 3D system.
The default is 'top'.</dd>
<dt><strong><code>palette</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>List of colors when coloring particles according to a discrete property,
such as 'species' or 'label'. A default palette will be used if not
specified. The default is None.</dd>
<dt><strong><code>cmap</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of a matplotlib colormap to use when coloring particles according
to a continuous property such as 'velocity' or 'energy'. List of
available colormap can be found in <code>matplotlib.cm.cmaps_listed</code>.
The default is 'viridis'.</dd>
<dt><strong><code>outfile</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Output filename to save the snapshot. The default is None (not saved).</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code>, optional</dt>
<dd>Size of the image to render. The default is (640, 480).</dd>
<dt><strong><code>zoom</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Zoom on the simulation box. The default is True.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Image</code></dt>
<dd>Rendered image.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_ovito(system, color, view=&#39;top&#39;, palette=None, cmap=&#39;viridis&#39;,
               outfile=None, size=(640, 480), zoom=True):
    &#34;&#34;&#34;
    Make a snapshot of the `system` using Ovito.
    The image is returned for further customization or visualization 
    in jupyter notebooks.        

    Parameters
    ----------
    system : System
        An instance of `System`.
    color : str
        Particle property to use for color coding, e.g. &#39;species&#39;, &#39;label&#39;.
    view : str, optional
        View type, i.e. face of the box to show. Only works for a 3D system.
        The default is &#39;top&#39;.
    palette : list, optional
        List of colors when coloring particles according to a discrete property,
        such as &#39;species&#39; or &#39;label&#39;. A default palette will be used if not 
        specified. The default is None.
    cmap : str, optional
        Name of a matplotlib colormap to use when coloring particles according
        to a continuous property such as &#39;velocity&#39; or &#39;energy&#39;. List of 
        available colormap can be found in `matplotlib.cm.cmaps_listed`.
        The default is &#39;viridis&#39;.
    outfile : str, optional
        Output filename to save the snapshot. The default is None (not saved).
    size : tuple, optional
        Size of the image to render. The default is (640, 480).
    zoom : bool, optional
        Zoom on the simulation box. The default is True.

    Returns
    -------
    Image
        Rendered image.

    &#34;&#34;&#34;
    try:
        from ovito.io import import_file
    except ImportError:
        print(&#39;install ovito to display the particles&#39;)
        return
    from ovito.vis import Viewport, TachyonRenderer
    import os
    import tempfile
    from .core.utils import tipify
    from matplotlib.cm import cmaps_listed

    # discrete property?
    property_vals = system.get_property(&#39;particle.{}&#39;.format(color))
    discrete = isinstance(tipify(str(property_vals[0])), (str, int))
    # corresponding color system
    discrete_colors = _palette if palette is None else palette
    colormap = cmaps_listed[cmap]

    # discrete or continuous color field
    if palette is None:
        discrete_colors = _palette
        discrete_colors = [hex_to_rgb(c) for c in discrete_colors]
    else:
        discrete_colors = palette
    # corresponding palette / colormap
    if discrete:
        property_set = list(set(property_vals))
        property_set.sort()
        color_db = discrete_colors
    else:
        color_db = colormap(property_vals)
    
    # individual particle color
    for pn, p in enumerate(system.particle):
        if discrete:
            p_color = color_db[property_set.index(p.__getattribute__(color))]
            p.color = p_color
        else:
            p.color = color_db[pn]
    
    # Get a temporary file to write the sample
    fh = tempfile.NamedTemporaryFile(&#39;w&#39;, suffix=&#39;.xyz&#39;, delete=False)
    tmp_file = fh.name
    # Self-contained EXYZ dump (it is not clean to use trajectories here)
    fh.write(&#39;{}\n&#39;.format(len(system.particle)))
    fh.write(&#39;Properties=species:S:1:pos:R:3:radius:R:1:color:R:3 Lattice=&#34;{},0.,0.,0.,{},0.,0.,0.,{}&#34;\n&#39;.format(*system.cell.side))
    for p in system.particle:
        fh.write(&#39;{} {} {} {} {} {} {} {}\n&#39;.format(p.species, *p.position, p.radius, *p.color))
    fh.close()

    # Ovito stuff. Can be customized by client code.
    pipeline = import_file(tmp_file)
    # Ovito seems to ignore the lattice info of exyz file
    # so we forcibly set the cell info here
    pipeline.source.data.cell_[0, 0] = system.cell.side[0]
    pipeline.source.data.cell_[1, 1] = system.cell.side[1]
    pipeline.source.data.cell_[2, 2] = system.cell.side[2]
    pipeline.source.data.cell_[:, 3] = -system.cell.side/2
    pipeline.add_to_scene()

    views = {&#39;top&#39;:    Viewport.Type.Top,
             &#39;bottom&#39;: Viewport.Type.Bottom,
             &#39;front&#39;:  Viewport.Type.Front,
             &#39;back&#39;:   Viewport.Type.Back,
             &#39;left&#39;:   Viewport.Type.Left,
             &#39;right&#39;:  Viewport.Type.Right}
    vp_type = views[view]
    vp = Viewport(type=vp_type)

    # Render
    if zoom:
        vp.zoom_all()
    if outfile is None:
        outfile = tmp_file + &#39;.png&#39;
        
    vp.render_image(filename=outfile, 
                    size=size, 
                    renderer=TachyonRenderer())

    # Scene is a singleton, so we must clear it
    pipeline.remove_from_scene()
    
    # remove temporary exyz file
    os.remove(tmp_file)

    # Try to display the image (e.g. in a jupyter notebook)
    try:
        from IPython.display import Image
        return Image(outfile)
    except ImportError:
        return outfile</code></pre>
</details>
</dd>
<dt id="partycls.helpers.silhouette_samples"><code class="name flex">
<span>def <span class="ident">silhouette_samples</span></span>(<span>X, labels, *, metric='euclidean', **kwds)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the Silhouette Coefficient for each sample.</p>
<p>The Silhouette Coefficient is a measure of how well samples are clustered
with samples that are similar to themselves. Clustering models with a high
Silhouette Coefficient are said to be dense, where samples in the same
cluster are similar to each other, and well separated, where samples in
different clusters are not very similar to each other.</p>
<p>The Silhouette Coefficient is calculated using the mean intra-cluster
distance (<code>a</code>) and the mean nearest-cluster distance (<code>b</code>) for each
sample.
The Silhouette Coefficient for a sample is <code>(b - a) / max(a,
b)</code>.
Note that Silhouette Coefficient is only defined if number of labels
is 2 <code>&lt;= n_labels &lt;= n_samples - 1</code>.</p>
<p>This function returns the Silhouette Coefficient for each sample.</p>
<p>The best value is 1 and the worst value is -1. Values near 0 indicate
overlapping clusters.</p>
<p>Read more in the :ref:<code>User Guide &lt;silhouette_coefficient&gt;</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples_a, n_samples_a) if metric ==
"precomputed"</code> or <code>(n_samples_a, n_features) otherwise</code></dt>
<dd>An array of pairwise distances between samples, or a feature array.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples,)</code></dt>
<dd>Label values for each sample.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code> or <code>callable</code>, default=<code>'euclidean'</code></dt>
<dd>The metric to use when calculating distance between instances in a
feature array. If metric is a string, it must be one of the options
allowed by :func:<code>sklearn.metrics.pairwise.pairwise_distances</code>.
If <code>X</code> is the distance array itself, use "precomputed" as the metric.
Precomputed distance matrices must have 0 along the diagonal.</dd>
</dl>
<p><code>**kwds</code> : optional keyword parameters
Any further parameters are passed directly to the distance function.
If using a <code>scipy.spatial.distance</code> metric, the parameters are still
metric dependent. See the scipy docs for usage examples.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>silhouette</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples,)</code></dt>
<dd>Silhouette Coefficients for each sample.</dd>
</dl>
<h2 id="references">References</h2>
<p>.. [1] <code>Peter J. Rousseeuw (1987). "Silhouettes: a Graphical Aid to the
Interpretation and Validation of Cluster Analysis". Computational
and Applied Mathematics 20: 53-65.
&lt;https://www.sciencedirect.com/science/article/pii/0377042787901257&gt;</code>_</p>
<p>.. [2] <code>Wikipedia entry on the Silhouette Coefficient
&lt;https://en.wikipedia.org/wiki/Silhouette_(clustering)&gt;</code>_</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_deprecate_positional_args
def silhouette_samples(X, labels, *, metric=&#39;euclidean&#39;, **kwds):
    &#34;&#34;&#34;Compute the Silhouette Coefficient for each sample.

    The Silhouette Coefficient is a measure of how well samples are clustered
    with samples that are similar to themselves. Clustering models with a high
    Silhouette Coefficient are said to be dense, where samples in the same
    cluster are similar to each other, and well separated, where samples in
    different clusters are not very similar to each other.

    The Silhouette Coefficient is calculated using the mean intra-cluster
    distance (``a``) and the mean nearest-cluster distance (``b``) for each
    sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,
    b)``.
    Note that Silhouette Coefficient is only defined if number of labels
    is 2 ``&lt;= n_labels &lt;= n_samples - 1``.

    This function returns the Silhouette Coefficient for each sample.

    The best value is 1 and the worst value is -1. Values near 0 indicate
    overlapping clusters.

    Read more in the :ref:`User Guide &lt;silhouette_coefficient&gt;`.

    Parameters
    ----------
    X : array-like of shape (n_samples_a, n_samples_a) if metric == \
            &#34;precomputed&#34; or (n_samples_a, n_features) otherwise
        An array of pairwise distances between samples, or a feature array.

    labels : array-like of shape (n_samples,)
        Label values for each sample.

    metric : str or callable, default=&#39;euclidean&#39;
        The metric to use when calculating distance between instances in a
        feature array. If metric is a string, it must be one of the options
        allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`.
        If ``X`` is the distance array itself, use &#34;precomputed&#34; as the metric.
        Precomputed distance matrices must have 0 along the diagonal.

    `**kwds` : optional keyword parameters
        Any further parameters are passed directly to the distance function.
        If using a ``scipy.spatial.distance`` metric, the parameters are still
        metric dependent. See the scipy docs for usage examples.

    Returns
    -------
    silhouette : array-like of shape (n_samples,)
        Silhouette Coefficients for each sample.

    References
    ----------

    .. [1] `Peter J. Rousseeuw (1987). &#34;Silhouettes: a Graphical Aid to the
       Interpretation and Validation of Cluster Analysis&#34;. Computational
       and Applied Mathematics 20: 53-65.
       &lt;https://www.sciencedirect.com/science/article/pii/0377042787901257&gt;`_

    .. [2] `Wikipedia entry on the Silhouette Coefficient
       &lt;https://en.wikipedia.org/wiki/Silhouette_(clustering)&gt;`_

    &#34;&#34;&#34;
    X, labels = check_X_y(X, labels, accept_sparse=[&#39;csc&#39;, &#39;csr&#39;])

    # Check for non-zero diagonal entries in precomputed distance matrix
    if metric == &#39;precomputed&#39;:
        atol = np.finfo(X.dtype).eps * 100
        if np.any(np.abs(np.diagonal(X)) &gt; atol):
            raise ValueError(
                &#39;The precomputed distance matrix contains non-zero &#39;
                &#39;elements on the diagonal. Use np.fill_diagonal(X, 0).&#39;
            )

    le = LabelEncoder()
    labels = le.fit_transform(labels)
    n_samples = len(labels)
    label_freqs = np.bincount(labels)
    check_number_of_labels(len(le.classes_), n_samples)

    kwds[&#39;metric&#39;] = metric
    reduce_func = functools.partial(_silhouette_reduce,
                                    labels=labels, label_freqs=label_freqs)
    results = zip(*pairwise_distances_chunked(X, reduce_func=reduce_func,
                                              **kwds))
    intra_clust_dists, inter_clust_dists = results
    intra_clust_dists = np.concatenate(intra_clust_dists)
    inter_clust_dists = np.concatenate(inter_clust_dists)

    denom = (label_freqs - 1).take(labels, mode=&#39;clip&#39;)
    with np.errstate(divide=&#34;ignore&#34;, invalid=&#34;ignore&#34;):
        intra_clust_dists /= denom

    sil_samples = inter_clust_dists - intra_clust_dists
    with np.errstate(divide=&#34;ignore&#34;, invalid=&#34;ignore&#34;):
        sil_samples /= np.maximum(intra_clust_dists, inter_clust_dists)
    # nan values are for clusters of size 1, and should be 0
    return np.nan_to_num(sil_samples)</code></pre>
</details>
</dd>
<dt id="partycls.helpers.silhouette_score"><code class="name flex">
<span>def <span class="ident">silhouette_score</span></span>(<span>X, labels, *, metric='euclidean', sample_size=None, random_state=None, **kwds)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the mean Silhouette Coefficient of all samples.</p>
<p>The Silhouette Coefficient is calculated using the mean intra-cluster
distance (<code>a</code>) and the mean nearest-cluster distance (<code>b</code>) for each
sample.
The Silhouette Coefficient for a sample is <code>(b - a) / max(a,
b)&lt;code&gt;.
To clarify, &lt;/code&gt;b</code> is the distance between a sample and the nearest
cluster that the sample is not a part of.
Note that Silhouette Coefficient is only defined if number of labels
is <code>2 &lt;= n_labels &lt;= n_samples - 1</code>.</p>
<p>This function returns the mean Silhouette Coefficient over all samples.
To obtain the values for each sample, use :func:<code><a title="partycls.helpers.silhouette_samples" href="#partycls.helpers.silhouette_samples">silhouette_samples()</a></code>.</p>
<p>The best value is 1 and the worst value is -1. Values near 0 indicate
overlapping clusters. Negative values generally indicate that a sample has
been assigned to the wrong cluster, as a different cluster is more similar.</p>
<p>Read more in the :ref:<code>User Guide &lt;silhouette_coefficient&gt;</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples_a, n_samples_a) if metric ==
"precomputed"</code> or <code>(n_samples_a, n_features) otherwise</code></dt>
<dd>An array of pairwise distances between samples, or a feature array.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples,)</code></dt>
<dd>Predicted labels for each sample.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code> or <code>callable</code>, default=<code>'euclidean'</code></dt>
<dd>The metric to use when calculating distance between instances in a
feature array. If metric is a string, it must be one of the options
allowed by :func:<code>metrics.pairwise.pairwise_distances
&lt;sklearn.metrics.pairwise.pairwise_distances&gt;</code>. If <code>X</code> is
the distance array itself, use <code>metric="precomputed"</code>.</dd>
<dt><strong><code>sample_size</code></strong> :&ensp;<code>int</code>, default=<code>None</code></dt>
<dd>The size of the sample to use when computing the Silhouette Coefficient
on a random subset of the data.
If <code>sample_size is None</code>, no sampling is used.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int, RandomState instance</code> or <code>None</code>, default=<code>None</code></dt>
<dd>Determines random number generation for selecting a subset of samples.
Used when <code>sample_size is not None</code>.
Pass an int for reproducible results across multiple function calls.
See :term:<code>Glossary &lt;random_state&gt;</code>.</dd>
<dt><strong><code>**kwds</code></strong> :&ensp;<code>optional keyword parameters</code></dt>
<dd>Any further parameters are passed directly to the distance function.
If using a scipy.spatial.distance metric, the parameters are still
metric dependent. See the scipy docs for usage examples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>silhouette</code></strong> :&ensp;<code>float</code></dt>
<dd>Mean Silhouette Coefficient for all samples.</dd>
</dl>
<h2 id="references">References</h2>
<p>.. [1] <code>Peter J. Rousseeuw (1987). "Silhouettes: a Graphical Aid to the
Interpretation and Validation of Cluster Analysis". Computational
and Applied Mathematics 20: 53-65.
&lt;https://www.sciencedirect.com/science/article/pii/0377042787901257&gt;</code>_</p>
<p>.. [2] <code>Wikipedia entry on the Silhouette Coefficient
&lt;https://en.wikipedia.org/wiki/Silhouette_(clustering)&gt;</code>_</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@_deprecate_positional_args
def silhouette_score(X, labels, *, metric=&#39;euclidean&#39;, sample_size=None,
                     random_state=None, **kwds):
    &#34;&#34;&#34;Compute the mean Silhouette Coefficient of all samples.

    The Silhouette Coefficient is calculated using the mean intra-cluster
    distance (``a``) and the mean nearest-cluster distance (``b``) for each
    sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,
    b)``.  To clarify, ``b`` is the distance between a sample and the nearest
    cluster that the sample is not a part of.
    Note that Silhouette Coefficient is only defined if number of labels
    is ``2 &lt;= n_labels &lt;= n_samples - 1``.

    This function returns the mean Silhouette Coefficient over all samples.
    To obtain the values for each sample, use :func:`silhouette_samples`.

    The best value is 1 and the worst value is -1. Values near 0 indicate
    overlapping clusters. Negative values generally indicate that a sample has
    been assigned to the wrong cluster, as a different cluster is more similar.

    Read more in the :ref:`User Guide &lt;silhouette_coefficient&gt;`.

    Parameters
    ----------
    X : array-like of shape (n_samples_a, n_samples_a) if metric == \
            &#34;precomputed&#34; or (n_samples_a, n_features) otherwise
        An array of pairwise distances between samples, or a feature array.

    labels : array-like of shape (n_samples,)
        Predicted labels for each sample.

    metric : str or callable, default=&#39;euclidean&#39;
        The metric to use when calculating distance between instances in a
        feature array. If metric is a string, it must be one of the options
        allowed by :func:`metrics.pairwise.pairwise_distances
        &lt;sklearn.metrics.pairwise.pairwise_distances&gt;`. If ``X`` is
        the distance array itself, use ``metric=&#34;precomputed&#34;``.

    sample_size : int, default=None
        The size of the sample to use when computing the Silhouette Coefficient
        on a random subset of the data.
        If ``sample_size is None``, no sampling is used.

    random_state : int, RandomState instance or None, default=None
        Determines random number generation for selecting a subset of samples.
        Used when ``sample_size is not None``.
        Pass an int for reproducible results across multiple function calls.
        See :term:`Glossary &lt;random_state&gt;`.

    **kwds : optional keyword parameters
        Any further parameters are passed directly to the distance function.
        If using a scipy.spatial.distance metric, the parameters are still
        metric dependent. See the scipy docs for usage examples.

    Returns
    -------
    silhouette : float
        Mean Silhouette Coefficient for all samples.

    References
    ----------

    .. [1] `Peter J. Rousseeuw (1987). &#34;Silhouettes: a Graphical Aid to the
       Interpretation and Validation of Cluster Analysis&#34;. Computational
       and Applied Mathematics 20: 53-65.
       &lt;https://www.sciencedirect.com/science/article/pii/0377042787901257&gt;`_

    .. [2] `Wikipedia entry on the Silhouette Coefficient
           &lt;https://en.wikipedia.org/wiki/Silhouette_(clustering)&gt;`_

    &#34;&#34;&#34;
    if sample_size is not None:
        X, labels = check_X_y(X, labels, accept_sparse=[&#39;csc&#39;, &#39;csr&#39;])
        random_state = check_random_state(random_state)
        indices = random_state.permutation(X.shape[0])[:sample_size]
        if metric == &#34;precomputed&#34;:
            X, labels = X[indices].T[indices].T, labels[indices]
        else:
            X, labels = X[indices], labels[indices]
    return np.mean(silhouette_samples(X, labels, metric=metric, **kwds))</code></pre>
</details>
</dd>
<dt id="partycls.helpers.sort_clusters"><code class="name flex">
<span>def <span class="ident">sort_clusters</span></span>(<span>labels, centroids, func=&lt;function shannon_entropy&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Make a consistent labeling of the clusters based on their centroids by
computing an associated numerical value as sorting criterion. By default,
the labeling is based on the Shannon entropy of each cluster.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>Original labels.</dd>
<dt><strong><code>centroids</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Cluster centroids.</dd>
<dt><strong><code>func</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>Function used to associate a numerical value to each cluster, to be
used as sorting criterion. This function must accept a list or a
one dimensional array as parameter (this parameter being the
coordinates of a given centroid). The default is shannon_entropy.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>new_labels</code></strong> :&ensp;<code>list</code></dt>
<dd>New labels based on centroid entropies.</dd>
<dt><strong><code>new_centroids</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Centroids arranged in order of descending entropies.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sort_clusters(labels, centroids, func=shannon_entropy):
    &#34;&#34;&#34;
    Make a consistent labeling of the clusters based on their centroids by
    computing an associated numerical value as sorting criterion. By default, 
    the labeling is based on the Shannon entropy of each cluster.

    Parameters
    ----------
    labels : list
        Original labels.
    centroids : numpy.ndarray
        Cluster centroids.
    func : function, optional
        Function used to associate a numerical value to each cluster, to be
        used as sorting criterion. This function must accept a list or a 
        one dimensional array as parameter (this parameter being the 
        coordinates of a given centroid). The default is shannon_entropy.

    Returns
    -------
    new_labels : list
        New labels based on centroid entropies.
    new_centroids : numpy.ndarray
        Centroids arranged in order of descending entropies.

    &#34;&#34;&#34;
    n_clusters = centroids.shape[0]
    new_centroids = numpy.zeros_like(centroids)
    new_labels = labels.copy()
    entropies = numpy.empty(n_clusters)
    # Shannon entropy of each cluster based on centroids
    for k in range(n_clusters):
        entropies[k] = func(centroids[k])
    # rank distributions according to their entropies
    entropies *= -1
    ranks = numpy.argsort(numpy.argsort(entropies))
    # Change particles
    for i in range(len(labels)):
        k_i = labels[i]
        k_new = ranks[k_i]
        new_labels[i] = k_new
    # change fractions, hist and dist
    for k in range(n_clusters):
        k_new = ranks[k]
        new_centroids[k_new] = centroids[k]
    return new_labels, new_centroids</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="partycls" href="index.html">partycls</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="partycls.helpers.AMI" href="#partycls.helpers.AMI">AMI</a></code></li>
<li><code><a title="partycls.helpers.ARI" href="#partycls.helpers.ARI">ARI</a></code></li>
<li><code><a title="partycls.helpers.merge_clusters" href="#partycls.helpers.merge_clusters">merge_clusters</a></code></li>
<li><code><a title="partycls.helpers.shannon_entropy" href="#partycls.helpers.shannon_entropy">shannon_entropy</a></code></li>
<li><code><a title="partycls.helpers.show_3dmol" href="#partycls.helpers.show_3dmol">show_3dmol</a></code></li>
<li><code><a title="partycls.helpers.show_matplotlib" href="#partycls.helpers.show_matplotlib">show_matplotlib</a></code></li>
<li><code><a title="partycls.helpers.show_ovito" href="#partycls.helpers.show_ovito">show_ovito</a></code></li>
<li><code><a title="partycls.helpers.silhouette_samples" href="#partycls.helpers.silhouette_samples">silhouette_samples</a></code></li>
<li><code><a title="partycls.helpers.silhouette_score" href="#partycls.helpers.silhouette_score">silhouette_score</a></code></li>
<li><code><a title="partycls.helpers.sort_clusters" href="#partycls.helpers.sort_clusters">sort_clusters</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>