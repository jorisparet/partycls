<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>partycls.dim_reduction API documentation</title>
<meta name="description" content="Dimensionality reduction techniques (linear and non-linear), to be performed
on a dataset stored in a numpy array." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<style>.homelink{display:block}.homelink img{max-width:150px;margin-left:auto;margin-right:auto;margin-bottom:.3em}</style>
<link rel="shortcut icon" href="https://raw.githubusercontent.com/jorisparet/partycls/jupyter-book/logo/favicon.svg"/>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>partycls.dim_reduction</code></h1>
</header>
<section id="section-intro">
<p>Dimensionality reduction techniques (linear and non-linear), to be performed
on a dataset stored in a numpy array.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Dimensionality reduction techniques (linear and non-linear), to be performed
on a dataset stored in a numpy array.
&#34;&#34;&#34;

import numpy
from sklearn.decomposition import PCA as _PCA
from sklearn.manifold import TSNE as _TSNE
from sklearn.manifold import LocallyLinearEmbedding as _LocallyLinearEmbedding
from sklearn.neural_network import MLPRegressor

__all__ = [&#39;PCA&#39;, &#39;TSNE&#39;, &#39;LocallyLinearEmbedding&#39;, &#39;AutoEncoder&#39;]


class PCA(_PCA):

    symbol = &#39;pca&#39;
    full_name = &#39;Principal Component Analysis (PCA)&#39;

    def reduce(self, X):
        &#34;&#34;&#34;
        Project the input features onto a reduced space using principal
        component analysis.

        Parameters
        ----------
        X : numpy.ndarray
            Features in the original space.

        Returns
        -------
        numpy.ndarray
            Features in the reduced space.

        &#34;&#34;&#34;
        return self.fit_transform(X)


class TSNE(_TSNE):

    symbol = &#39;tsne&#39;
    full_name = &#39;t-distributed Stochastic Neighbor Embedding (t-SNE)&#39;

    def reduce(self, X):
        &#34;&#34;&#34;
        Project the input features onto a reduced space using t-distributed 
        stochastic neighbor embedding.

        Parameters
        ----------
        X : numpy.ndarray
            Features in the original space.

        Returns
        -------
        numpy.ndarray
            Features in the reduced space.

        &#34;&#34;&#34;
        return self.fit_transform(X)


class LocallyLinearEmbedding(_LocallyLinearEmbedding):

    symbol = &#39;lle&#39;
    full_name = &#39;Locally Linear Embedding (LLE)&#39;

    def reduce(self, X):
        &#34;&#34;&#34;
        Project the input features onto a reduced space using locally
        linear embedding.

        Parameters
        ----------
        X : numpy.ndarray
            Features in the original space.

        Returns
        -------
        numpy.ndarray
            Features in the reduced space.

        &#34;&#34;&#34;
        return self.fit_transform(X)


class AutoEncoder(MLPRegressor):

    symbol = &#39;ae&#39;
    full_name = &#39;Neural-Network Auto-Encoder (AE)&#39;

    def __init__(self, layers=(100, 2, 100), activation=&#39;relu&#39;, solver=&#39;adam&#39;, alpha=1e-4):
        MLPRegressor.__init__(self, hidden_layer_sizes=layers,
                              activation=activation, solver=solver,
                              alpha=alpha)

    @property
    def n_components(self):
        return min(self.hidden_layer_sizes)

    def reduce(self, X):
        &#34;&#34;&#34;
        Project the input features onto a reduced space using a neural network
        autoencoder. The dimension of the reduced space is the number of 
        nodes in the bottleneck layer.

        Parameters
        ----------
        X : numpy.ndarray
            Features in the original space.

        Returns
        -------
        numpy.ndarray
            Features in the reduced space.

        &#34;&#34;&#34;

        # Train the network to reproduce its input as output
        self.fit(X, X)

        # Mean absolute error
        Y_pred = self.predict(X)
        # MAE
        MAE = numpy.abs(Y_pred - X).mean()
        self.mean_absolute_error = MAE
        # MSE / MSD
        MSE = 0.0
        MSD = 0.0
        Xmean = numpy.mean(X, axis=0)
        for i in range(X.shape[0]):
            MSE += numpy.sum((X[i] - self.predict(X[i].reshape(1, -1)))**2)
            MSD += numpy.sum((X[i] - Xmean)**2)
        MSE /= X.shape[0]
        MSD /= X.shape[0]
        self.mean_squared_error = MSE
        self.mean_squared_deviation = MSD

        # Weights and biases
        W = self.coefs_
        biases = self.intercepts_

        # Keep the encoder part only
        bottleneck_index = self.hidden_layer_sizes.index(self.n_components)
        encoder_weights = W[0:bottleneck_index + 1]
        encoder_biases = biases[0:bottleneck_index + 1]

        # Encode data
        X_red = X
        for index, (w, b) in enumerate(zip(encoder_weights, encoder_biases)):
            if index + 1 == len(encoder_weights):
                X_red = X_red @ w + b
            else:
                # Use the right activation function here
                if self.activation == &#39;relu&#39;:
                    X_red = numpy.maximum(0, X_red @ w + b)
                if self.activation == &#39;tanh&#39;:
                    X_red = numpy.tanh(X_red @ w + b)
                if self.activation == &#39;identity&#39;:
                    X_red = X_red @ w + b
                if self.activation == &#39;logistic&#39;:
                    X_red = 1.0 / (1.0 + numpy.exp(-(X_red @ w + b)))

        # Return the dataset in low dimension
        return X_red</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="partycls.dim_reduction.AutoEncoder"><code class="flex name class">
<span>class <span class="ident">AutoEncoder</span></span>
<span>(</span><span>layers=(100, 2, 100), activation='relu', solver='adam', alpha=0.0001)</span>
</code></dt>
<dd>
<div class="desc"><p>Multi-layer Perceptron regressor.</p>
<p>This model optimizes the squared-loss using LBFGS or stochastic gradient
descent.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.18</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>hidden_layer_sizes</code></strong> :&ensp;<code>tuple, length = n_layers - 2</code>, default=<code>(100,)</code></dt>
<dd>The ith element represents the number of neurons in the ith
hidden layer.</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>{'identity', 'logistic', 'tanh', 'relu'}</code>, default=<code>'relu'</code></dt>
<dd>
<p>Activation function for the hidden layer.</p>
<ul>
<li>
<p>'identity', no-op activation, useful to implement linear bottleneck,
returns f(x) = x</p>
</li>
<li>
<p>'logistic', the logistic sigmoid function,
returns f(x) = 1 / (1 + exp(-x)).</p>
</li>
<li>
<p>'tanh', the hyperbolic tan function,
returns f(x) = tanh(x).</p>
</li>
<li>
<p>'relu', the rectified linear unit function,
returns f(x) = max(0, x)</p>
</li>
</ul>
</dd>
<dt><strong><code>solver</code></strong> :&ensp;<code>{'lbfgs', 'sgd', 'adam'}</code>, default=<code>'adam'</code></dt>
<dd>
<p>The solver for weight optimization.</p>
<ul>
<li>
<p>'lbfgs' is an optimizer in the family of quasi-Newton methods.</p>
</li>
<li>
<p>'sgd' refers to stochastic gradient descent.</p>
</li>
<li>
<p>'adam' refers to a stochastic gradient-based optimizer proposed by
Kingma, Diederik, and Jimmy Ba</p>
</li>
</ul>
<p>Note: The default solver 'adam' works pretty well on relatively
large datasets (with thousands of training samples or more) in terms of
both training time and validation score.
For small datasets, however, 'lbfgs' can converge faster and perform
better.</p>
</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code>, default=<code>0.0001</code></dt>
<dd>L2 penalty (regularization term) parameter.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, default=<code>'auto'</code></dt>
<dd>Size of minibatches for stochastic optimizers.
If the solver is 'lbfgs', the classifier will not use minibatch.
When set to "auto", <code>batch_size=min(200, n_samples)</code></dd>
<dt><strong><code>learning_rate</code></strong> :&ensp;<code>{'constant', 'invscaling', 'adaptive'}</code>, default=<code>'constant'</code></dt>
<dd>
<p>Learning rate schedule for weight updates.</p>
<ul>
<li>
<p>'constant' is a constant learning rate given by
'learning_rate_init'.</p>
</li>
<li>
<p>'invscaling' gradually decreases the learning rate <code>learning_rate_</code>
at each time step 't' using an inverse scaling exponent of 'power_t'.
effective_learning_rate = learning_rate_init / pow(t, power_t)</p>
</li>
<li>
<p>'adaptive' keeps the learning rate constant to
'learning_rate_init' as long as training loss keeps decreasing.
Each time two consecutive epochs fail to decrease training loss by at
least tol, or fail to increase validation score by at least tol if
'early_stopping' is on, the current learning rate is divided by 5.</p>
</li>
</ul>
<p>Only used when solver='sgd'.</p>
</dd>
<dt><strong><code>learning_rate_init</code></strong> :&ensp;<code>double</code>, default=<code>0.001</code></dt>
<dd>The initial learning rate used. It controls the step-size
in updating the weights. Only used when solver='sgd' or 'adam'.</dd>
<dt><strong><code>power_t</code></strong> :&ensp;<code>double</code>, default=<code>0.5</code></dt>
<dd>The exponent for inverse scaling learning rate.
It is used in updating effective learning rate when the learning_rate
is set to 'invscaling'. Only used when solver='sgd'.</dd>
<dt><strong><code>max_iter</code></strong> :&ensp;<code>int</code>, default=<code>200</code></dt>
<dd>Maximum number of iterations. The solver iterates until convergence
(determined by 'tol') or this number of iterations. For stochastic
solvers ('sgd', 'adam'), note that this determines the number of epochs
(how many times each data point will be used), not the number of
gradient steps.</dd>
<dt><strong><code>shuffle</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to shuffle samples in each iteration. Only used when
solver='sgd' or 'adam'.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int, RandomState instance</code>, default=<code>None</code></dt>
<dd>Determines random number generation for weights and bias
initialization, train-test split if early stopping is used, and batch
sampling when solver='sgd' or 'adam'.
Pass an int for reproducible results across multiple function calls.
See :term:<code>Glossary &lt;random_state&gt;</code>.</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code>, default=<code>1e-4</code></dt>
<dd>Tolerance for the optimization. When the loss or score is not improving
by at least <code>tol</code> for <code>n_iter_no_change</code> consecutive iterations,
unless <code>learning_rate</code> is set to 'adaptive', convergence is
considered to be reached and training stops.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to print progress messages to stdout.</dd>
<dt><strong><code>warm_start</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>When set to True, reuse the solution of the previous
call to fit as initialization, otherwise, just erase the
previous solution. See :term:<code>the Glossary &lt;warm_start&gt;</code>.</dd>
<dt><strong><code>momentum</code></strong> :&ensp;<code>float</code>, default=<code>0.9</code></dt>
<dd>Momentum for gradient descent update.
Should be between 0 and 1. Only
used when solver='sgd'.</dd>
<dt><strong><code>nesterovs_momentum</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to use Nesterov's momentum. Only used when solver='sgd' and
momentum &gt; 0.</dd>
<dt><strong><code>early_stopping</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to use early stopping to terminate training when validation
score is not improving. If set to true, it will automatically set
aside 10% of training data as validation and terminate training when
validation score is not improving by at least <code>tol</code> for
<code>n_iter_no_change</code> consecutive epochs.
Only effective when solver='sgd' or 'adam'</dd>
<dt><strong><code>validation_fraction</code></strong> :&ensp;<code>float</code>, default=<code>0.1</code></dt>
<dd>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True</dd>
<dt><strong><code>beta_1</code></strong> :&ensp;<code>float</code>, default=<code>0.9</code></dt>
<dd>Exponential decay rate for estimates of first moment vector in adam,
should be in [0, 1). Only used when solver='adam'</dd>
<dt><strong><code>beta_2</code></strong> :&ensp;<code>float</code>, default=<code>0.999</code></dt>
<dd>Exponential decay rate for estimates of second moment vector in adam,
should be in [0, 1). Only used when solver='adam'</dd>
<dt><strong><code>epsilon</code></strong> :&ensp;<code>float</code>, default=<code>1e-8</code></dt>
<dd>Value for numerical stability in adam. Only used when solver='adam'</dd>
<dt><strong><code>n_iter_no_change</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>
<p>Maximum number of epochs to not meet <code>tol</code> improvement.
Only effective when solver='sgd' or 'adam'</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.20</p>
</div>
</dd>
<dt><strong><code>max_fun</code></strong> :&ensp;<code>int</code>, default=<code>15000</code></dt>
<dd>
<p>Only used when solver='lbfgs'. Maximum number of function calls.
The solver iterates until convergence (determined by 'tol'), number
of iterations reaches max_iter, or this number of function calls.
Note that number of function calls will be greater than or equal to
the number of iterations for the MLPRegressor.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.22</p>
</div>
</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>loss_</code></strong> :&ensp;<code>float</code></dt>
<dd>The current loss computed with the loss function.</dd>
<dt><strong><code>best_loss_</code></strong> :&ensp;<code>float</code></dt>
<dd>The minimum loss reached by the solver throughout fitting.</dd>
<dt><strong><code>loss_curve_</code></strong> :&ensp;<code>list</code> of <code>shape (</code>n_iter_<code>,)</code></dt>
<dd>The ith element in the list represents the loss at the ith iteration.</dd>
<dt><strong><code>t_</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of training samples seen by the solver during fitting.</dd>
<dt><strong><code>coefs_</code></strong> :&ensp;<code>list</code> of <code>shape (n_layers - 1,)</code></dt>
<dd>The ith element in the list represents the weight matrix corresponding
to layer i.</dd>
<dt><strong><code>intercepts_</code></strong> :&ensp;<code>list</code> of <code>shape (n_layers - 1,)</code></dt>
<dd>The ith element in the list represents the bias vector corresponding to
layer i + 1.</dd>
<dt><strong><code>n_iter_</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of iterations the solver has ran.</dd>
<dt><strong><code>n_layers_</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of layers.</dd>
<dt><strong><code>n_outputs_</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of outputs.</dd>
<dt><strong><code>out_activation_</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the output activation function.</dd>
<dt><strong><code>loss_curve_</code></strong> :&ensp;<code>list</code> of <code>shape (n_iters,)</code></dt>
<dd>Loss value evaluated at the end of each training step.</dd>
<dt><strong><code>t_</code></strong> :&ensp;<code>int</code></dt>
<dd>Mathematically equals <code>n_iters * X.shape[0]</code>, it means
<code>time_step</code> and it is used by optimizer's learning rate scheduler.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from sklearn.neural_network import MLPRegressor
&gt;&gt;&gt; from sklearn.datasets import make_regression
&gt;&gt;&gt; from sklearn.model_selection import train_test_split
&gt;&gt;&gt; X, y = make_regression(n_samples=200, random_state=1)
&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y,
...                                                     random_state=1)
&gt;&gt;&gt; regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)
&gt;&gt;&gt; regr.predict(X_test[:2])
array([-0.9..., -7.1...])
&gt;&gt;&gt; regr.score(X_test, y_test)
0.4...
</code></pre>
<h2 id="notes">Notes</h2>
<p>MLPRegressor trains iteratively since at each time step
the partial derivatives of the loss function with respect to the model
parameters are computed to update the parameters.</p>
<p>It can also have a regularization term added to the loss function
that shrinks model parameters to prevent overfitting.</p>
<p>This implementation works with data represented as dense and sparse numpy
arrays of floating point values.</p>
<h2 id="references">References</h2>
<p>Hinton, Geoffrey E.
"Connectionist learning procedures." Artificial intelligence 40.1
(1989): 185-234.</p>
<p>Glorot, Xavier, and Yoshua Bengio. "Understanding the difficulty of
training deep feedforward neural networks." International Conference
on Artificial Intelligence and Statistics. 2010.</p>
<p>He, Kaiming, et al. "Delving deep into rectifiers: Surpassing human-level
performance on imagenet classification." arXiv preprint
arXiv:1502.01852 (2015).</p>
<p>Kingma, Diederik, and Jimmy Ba. "Adam: A method for stochastic
optimization." arXiv preprint arXiv:1412.6980 (2014).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoEncoder(MLPRegressor):

    symbol = &#39;ae&#39;
    full_name = &#39;Neural-Network Auto-Encoder (AE)&#39;

    def __init__(self, layers=(100, 2, 100), activation=&#39;relu&#39;, solver=&#39;adam&#39;, alpha=1e-4):
        MLPRegressor.__init__(self, hidden_layer_sizes=layers,
                              activation=activation, solver=solver,
                              alpha=alpha)

    @property
    def n_components(self):
        return min(self.hidden_layer_sizes)

    def reduce(self, X):
        &#34;&#34;&#34;
        Project the input features onto a reduced space using a neural network
        autoencoder. The dimension of the reduced space is the number of 
        nodes in the bottleneck layer.

        Parameters
        ----------
        X : numpy.ndarray
            Features in the original space.

        Returns
        -------
        numpy.ndarray
            Features in the reduced space.

        &#34;&#34;&#34;

        # Train the network to reproduce its input as output
        self.fit(X, X)

        # Mean absolute error
        Y_pred = self.predict(X)
        # MAE
        MAE = numpy.abs(Y_pred - X).mean()
        self.mean_absolute_error = MAE
        # MSE / MSD
        MSE = 0.0
        MSD = 0.0
        Xmean = numpy.mean(X, axis=0)
        for i in range(X.shape[0]):
            MSE += numpy.sum((X[i] - self.predict(X[i].reshape(1, -1)))**2)
            MSD += numpy.sum((X[i] - Xmean)**2)
        MSE /= X.shape[0]
        MSD /= X.shape[0]
        self.mean_squared_error = MSE
        self.mean_squared_deviation = MSD

        # Weights and biases
        W = self.coefs_
        biases = self.intercepts_

        # Keep the encoder part only
        bottleneck_index = self.hidden_layer_sizes.index(self.n_components)
        encoder_weights = W[0:bottleneck_index + 1]
        encoder_biases = biases[0:bottleneck_index + 1]

        # Encode data
        X_red = X
        for index, (w, b) in enumerate(zip(encoder_weights, encoder_biases)):
            if index + 1 == len(encoder_weights):
                X_red = X_red @ w + b
            else:
                # Use the right activation function here
                if self.activation == &#39;relu&#39;:
                    X_red = numpy.maximum(0, X_red @ w + b)
                if self.activation == &#39;tanh&#39;:
                    X_red = numpy.tanh(X_red @ w + b)
                if self.activation == &#39;identity&#39;:
                    X_red = X_red @ w + b
                if self.activation == &#39;logistic&#39;:
                    X_red = 1.0 / (1.0 + numpy.exp(-(X_red @ w + b)))

        # Return the dataset in low dimension
        return X_red</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.neural_network._multilayer_perceptron.MLPRegressor</li>
<li>sklearn.base.RegressorMixin</li>
<li>sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron</li>
<li>sklearn.base.BaseEstimator</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="partycls.dim_reduction.AutoEncoder.full_name"><code class="name">var <span class="ident">full_name</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="partycls.dim_reduction.AutoEncoder.symbol"><code class="name">var <span class="ident">symbol</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="partycls.dim_reduction.AutoEncoder.n_components"><code class="name">var <span class="ident">n_components</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def n_components(self):
    return min(self.hidden_layer_sizes)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="partycls.dim_reduction.AutoEncoder.reduce"><code class="name flex">
<span>def <span class="ident">reduce</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Project the input features onto a reduced space using a neural network
autoencoder. The dimension of the reduced space is the number of
nodes in the bottleneck layer.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Features in the original space.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Features in the reduced space.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reduce(self, X):
    &#34;&#34;&#34;
    Project the input features onto a reduced space using a neural network
    autoencoder. The dimension of the reduced space is the number of 
    nodes in the bottleneck layer.

    Parameters
    ----------
    X : numpy.ndarray
        Features in the original space.

    Returns
    -------
    numpy.ndarray
        Features in the reduced space.

    &#34;&#34;&#34;

    # Train the network to reproduce its input as output
    self.fit(X, X)

    # Mean absolute error
    Y_pred = self.predict(X)
    # MAE
    MAE = numpy.abs(Y_pred - X).mean()
    self.mean_absolute_error = MAE
    # MSE / MSD
    MSE = 0.0
    MSD = 0.0
    Xmean = numpy.mean(X, axis=0)
    for i in range(X.shape[0]):
        MSE += numpy.sum((X[i] - self.predict(X[i].reshape(1, -1)))**2)
        MSD += numpy.sum((X[i] - Xmean)**2)
    MSE /= X.shape[0]
    MSD /= X.shape[0]
    self.mean_squared_error = MSE
    self.mean_squared_deviation = MSD

    # Weights and biases
    W = self.coefs_
    biases = self.intercepts_

    # Keep the encoder part only
    bottleneck_index = self.hidden_layer_sizes.index(self.n_components)
    encoder_weights = W[0:bottleneck_index + 1]
    encoder_biases = biases[0:bottleneck_index + 1]

    # Encode data
    X_red = X
    for index, (w, b) in enumerate(zip(encoder_weights, encoder_biases)):
        if index + 1 == len(encoder_weights):
            X_red = X_red @ w + b
        else:
            # Use the right activation function here
            if self.activation == &#39;relu&#39;:
                X_red = numpy.maximum(0, X_red @ w + b)
            if self.activation == &#39;tanh&#39;:
                X_red = numpy.tanh(X_red @ w + b)
            if self.activation == &#39;identity&#39;:
                X_red = X_red @ w + b
            if self.activation == &#39;logistic&#39;:
                X_red = 1.0 / (1.0 + numpy.exp(-(X_red @ w + b)))

    # Return the dataset in low dimension
    return X_red</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="partycls.dim_reduction.LocallyLinearEmbedding"><code class="flex name class">
<span>class <span class="ident">LocallyLinearEmbedding</span></span>
<span>(</span><span>*, n_neighbors=5, n_components=2, reg=0.001, eigen_solver='auto', tol=1e-06, max_iter=100, method='standard', hessian_tol=0.0001, modified_tol=1e-12, neighbors_algorithm='auto', random_state=None, n_jobs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Locally Linear Embedding</p>
<p>Read more in the :ref:<code>User Guide &lt;locally_linear_embedding&gt;</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_neighbors</code></strong> :&ensp;<code>int</code>, default=<code>5</code></dt>
<dd>number of neighbors to consider for each point.</dd>
<dt><strong><code>n_components</code></strong> :&ensp;<code>int</code>, default=<code>2</code></dt>
<dd>number of coordinates for the manifold</dd>
<dt><strong><code>reg</code></strong> :&ensp;<code>float</code>, default=<code>1e-3</code></dt>
<dd>regularization constant, multiplies the trace of the local covariance
matrix of the distances.</dd>
<dt><strong><code>eigen_solver</code></strong> :&ensp;<code>{'auto', 'arpack', 'dense'}</code>, default=<code>'auto'</code></dt>
<dd>
<p>auto : algorithm will attempt to choose the best method for input data</p>
<p>arpack : use arnoldi iteration in shift-invert mode.
For this method, M may be a dense matrix, sparse matrix,
or general linear operator.
Warning: ARPACK can be unstable for some problems.
It is
best to try several random seeds in order to check results.</p>
<p>dense
: use standard dense matrix operations for the eigenvalue
decomposition.
For this method, M must be an array
or matrix type.
This method should be avoided for
large problems.</p>
</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code>, default=<code>1e-6</code></dt>
<dd>Tolerance for 'arpack' method
Not used if eigen_solver=='dense'.</dd>
<dt><strong><code>max_iter</code></strong> :&ensp;<code>int</code>, default=<code>100</code></dt>
<dd>maximum number of iterations for the arpack solver.
Not used if eigen_solver=='dense'.</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>{'standard', 'hessian', 'modified', 'ltsa'}</code>, default=<code>'standard'</code></dt>
<dd>standard : use the standard locally linear embedding algorithm.
see
reference [1]
hessian
: use the Hessian eigenmap method. This method requires
<code>n_neighbors &gt; n_components * (1 + (n_components + 1) / 2</code>
see reference [2]
modified : use the modified locally linear embedding algorithm.
see reference [3]
ltsa
: use local tangent space alignment algorithm
see reference [4]</dd>
<dt><strong><code>hessian_tol</code></strong> :&ensp;<code>float</code>, default=<code>1e-4</code></dt>
<dd>Tolerance for Hessian eigenmapping method.
Only used if <code>method == 'hessian'</code></dd>
<dt><strong><code>modified_tol</code></strong> :&ensp;<code>float</code>, default=<code>1e-12</code></dt>
<dd>Tolerance for modified LLE method.
Only used if <code>method == 'modified'</code></dd>
<dt><strong><code>neighbors_algorithm</code></strong> :&ensp;<code>{'auto', 'brute', 'kd_tree', 'ball_tree'}</code>,
default=<code>'auto'</code></dt>
<dd>algorithm to use for nearest neighbors search,
passed to neighbors.NearestNeighbors instance</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int, RandomState instance</code>, default=<code>None</code></dt>
<dd>Determines the random number generator when
<code>eigen_solver</code> == 'arpack'. Pass an int for reproducible results
across multiple function calls. See :term: <code>Glossary &lt;random_state&gt;</code>.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code> or <code>None</code>, default=<code>None</code></dt>
<dd>The number of parallel jobs to run.
<code>None</code> means 1 unless in a :obj:<code>joblib.parallel_backend</code> context.
<code>-1</code> means using all processors. See :term:<code>Glossary &lt;n_jobs&gt;</code>
for more details.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>embedding_</code></strong> :&ensp;<code>array-like, shape [n_samples, n_components]</code></dt>
<dd>Stores the embedding vectors</dd>
<dt><strong><code>reconstruction_error_</code></strong> :&ensp;<code>float</code></dt>
<dd>Reconstruction error associated with <code>embedding_</code></dd>
<dt><strong><code>nbrs_</code></strong> :&ensp;<code>NearestNeighbors object</code></dt>
<dd>Stores nearest neighbors instance, including BallTree or KDtree
if applicable.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from sklearn.datasets import load_digits
&gt;&gt;&gt; from sklearn.manifold import LocallyLinearEmbedding
&gt;&gt;&gt; X, _ = load_digits(return_X_y=True)
&gt;&gt;&gt; X.shape
(1797, 64)
&gt;&gt;&gt; embedding = LocallyLinearEmbedding(n_components=2)
&gt;&gt;&gt; X_transformed = embedding.fit_transform(X[:100])
&gt;&gt;&gt; X_transformed.shape
(100, 2)
</code></pre>
<h2 id="references">References</h2>
<p>.. [1] Roweis, S. &amp; Saul, L. Nonlinear dimensionality reduction
by locally linear embedding.
Science 290:2323 (2000).
.. [2] Donoho, D. &amp; Grimes, C. Hessian eigenmaps: Locally
linear embedding techniques for high-dimensional data.
Proc Natl Acad Sci U S A.
100:5591 (2003).
.. [3] Zhang, Z. &amp; Wang, J. MLLE: Modified Locally Linear
Embedding Using Multiple Weights.
<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382</a>
.. [4] Zhang, Z. &amp; Zha, H. Principal manifolds and nonlinear
dimensionality reduction via tangent space alignment.
Journal of Shanghai Univ.
8:406 (2004)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LocallyLinearEmbedding(_LocallyLinearEmbedding):

    symbol = &#39;lle&#39;
    full_name = &#39;Locally Linear Embedding (LLE)&#39;

    def reduce(self, X):
        &#34;&#34;&#34;
        Project the input features onto a reduced space using locally
        linear embedding.

        Parameters
        ----------
        X : numpy.ndarray
            Features in the original space.

        Returns
        -------
        numpy.ndarray
            Features in the reduced space.

        &#34;&#34;&#34;
        return self.fit_transform(X)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.manifold._locally_linear.LocallyLinearEmbedding</li>
<li>sklearn.base.TransformerMixin</li>
<li>sklearn.base._UnstableArchMixin</li>
<li>sklearn.base.BaseEstimator</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="partycls.dim_reduction.LocallyLinearEmbedding.full_name"><code class="name">var <span class="ident">full_name</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="partycls.dim_reduction.LocallyLinearEmbedding.symbol"><code class="name">var <span class="ident">symbol</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="partycls.dim_reduction.LocallyLinearEmbedding.reduce"><code class="name flex">
<span>def <span class="ident">reduce</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Project the input features onto a reduced space using locally
linear embedding.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Features in the original space.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Features in the reduced space.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reduce(self, X):
    &#34;&#34;&#34;
    Project the input features onto a reduced space using locally
    linear embedding.

    Parameters
    ----------
    X : numpy.ndarray
        Features in the original space.

    Returns
    -------
    numpy.ndarray
        Features in the reduced space.

    &#34;&#34;&#34;
    return self.fit_transform(X)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="partycls.dim_reduction.PCA"><code class="flex name class">
<span>class <span class="ident">PCA</span></span>
<span>(</span><span>n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Principal component analysis (PCA).</p>
<p>Linear dimensionality reduction using Singular Value Decomposition of the
data to project it to a lower dimensional space. The input data is centered
but not scaled for each feature before applying the SVD.</p>
<p>It uses the LAPACK implementation of the full SVD or a randomized truncated
SVD by the method of Halko et al. 2009, depending on the shape of the input
data and the number of components to extract.</p>
<p>It can also use the scipy.sparse.linalg ARPACK implementation of the
truncated SVD.</p>
<p>Notice that this class does not support sparse input. See
:class:<code>TruncatedSVD</code> for an alternative with sparse data.</p>
<p>Read more in the :ref:<code>User Guide &lt;PCA&gt;</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_components</code></strong> :&ensp;<code>int, float</code> or <code>'mle'</code>, default=<code>None</code></dt>
<dd>
<p>Number of components to keep.
if n_components is not set all components are kept::</p>
<pre><code>n_components == min(n_samples, n_features)
</code></pre>
<p>If <code>n_components == 'mle'</code> and <code>svd_solver == 'full'</code>, Minka's
MLE is used to guess the dimension. Use of <code>n_components == 'mle'</code>
will interpret <code>svd_solver == 'auto'</code> as <code>svd_solver == 'full'</code>.</p>
<p>If <code>0 &lt; n_components &lt; 1</code> and <code>svd_solver == 'full'</code>, select the
number of components such that the amount of variance that needs to be
explained is greater than the percentage specified by n_components.</p>
<p>If <code>svd_solver == 'arpack'</code>, the number of components must be
strictly less than the minimum of n_features and n_samples.</p>
<p>Hence, the None case results in::</p>
<pre><code>n_components == min(n_samples, n_features) - 1
</code></pre>
</dd>
<dt><strong><code>copy</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>If False, data passed to fit are overwritten and running
fit(X).transform(X) will not yield the expected results,
use fit_transform(X) instead.</dd>
<dt><strong><code>whiten</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>
<p>When True (False by default) the <code>components_</code> vectors are multiplied
by the square root of n_samples and then divided by the singular values
to ensure uncorrelated outputs with unit component-wise variances.</p>
<p>Whitening will remove some information from the transformed signal
(the relative variance scales of the components) but can sometime
improve the predictive accuracy of the downstream estimators by
making their data respect some hard-wired assumptions.</p>
</dd>
<dt><strong><code>svd_solver</code></strong> :&ensp;<code>{'auto', 'full', 'arpack', 'randomized'}</code>, default=<code>'auto'</code></dt>
<dd>
<p>If auto :
The solver is selected by a default policy based on <code>X.shape</code> and
<code>n_components</code>: if the input data is larger than 500x500 and the
number of components to extract is lower than 80% of the smallest
dimension of the data, then the more efficient 'randomized'
method is enabled. Otherwise the exact full SVD is computed and
optionally truncated afterwards.
If full :
run exact full SVD calling the standard LAPACK solver via
<code>scipy.linalg.svd</code> and select the components by postprocessing
If arpack :
run SVD truncated to n_components calling ARPACK solver via
<code>scipy.sparse.linalg.svds</code>. It requires strictly
0 &lt; n_components &lt; min(X.shape)
If randomized :
run randomized SVD by the method of Halko et al.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.18.0</p>
</div>
</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code>, default=<code>0.0</code></dt>
<dd>
<p>Tolerance for singular values computed by svd_solver == 'arpack'.
Must be of range [0.0, infinity).</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.18.0</p>
</div>
</dd>
<dt><strong><code>iterated_power</code></strong> :&ensp;<code>int</code> or <code>'auto'</code>, default=<code>'auto'</code></dt>
<dd>
<p>Number of iterations for the power method computed by
svd_solver == 'randomized'.
Must be of range [0, infinity).</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.18.0</p>
</div>
</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int, RandomState instance</code> or <code>None</code>, default=<code>None</code></dt>
<dd>
<p>Used when the 'arpack' or 'randomized' solvers are used. Pass an int
for reproducible results across multiple function calls.
See :term:<code>Glossary &lt;random_state&gt;</code>.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.18.0</p>
</div>
</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>components_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_components, n_features)</code></dt>
<dd>Principal axes in feature space, representing the directions of
maximum variance in the data. The components are sorted by
<code>explained_variance_</code>.</dd>
<dt><strong><code>explained_variance_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_components,)</code></dt>
<dd>
<p>The amount of variance explained by each of the selected components.</p>
<p>Equal to n_components largest eigenvalues
of the covariance matrix of X.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.18</p>
</div>
</dd>
<dt><strong><code>explained_variance_ratio_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_components,)</code></dt>
<dd>
<p>Percentage of variance explained by each of the selected components.</p>
<p>If <code>n_components</code> is not set then all components are stored and the
sum of the ratios is equal to 1.0.</p>
</dd>
<dt><strong><code>singular_values_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_components,)</code></dt>
<dd>
<p>The singular values corresponding to each of the selected components.
The singular values are equal to the 2-norms of the <code>n_components</code>
variables in the lower-dimensional space.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.19</p>
</div>
</dd>
<dt><strong><code>mean_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_features,)</code></dt>
<dd>
<p>Per-feature empirical mean, estimated from the training set.</p>
<p>Equal to <code>X.mean(axis=0)</code>.</p>
</dd>
<dt><strong><code>n_components_</code></strong> :&ensp;<code>int</code></dt>
<dd>The estimated number of components. When n_components is set
to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this
number is estimated from input data. Otherwise it equals the parameter
n_components, or the lesser value of n_features and n_samples
if n_components is None.</dd>
<dt><strong><code>n_features_</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of features in the training data.</dd>
<dt><strong><code>n_samples_</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples in the training data.</dd>
<dt><strong><code>noise_variance_</code></strong> :&ensp;<code>float</code></dt>
<dd>
<p>The estimated noise covariance following the Probabilistic PCA model
from Tipping and Bishop 1999. See "Pattern Recognition and
Machine Learning" by C. Bishop, 12.2.1 p. 574 or
<a href="http://www.miketipping.com/papers/met-mppca.pdf.">http://www.miketipping.com/papers/met-mppca.pdf.</a> It is required to
compute the estimated data covariance and score samples.</p>
<p>Equal to the average of (min(n_features, n_samples) - n_components)
smallest eigenvalues of the covariance matrix of X.</p>
</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>KernelPCA</code></dt>
<dd>Kernel Principal Component Analysis.</dd>
<dt><code>SparsePCA</code></dt>
<dd>Sparse Principal Component Analysis.</dd>
<dt><code>TruncatedSVD</code></dt>
<dd>Dimensionality reduction using truncated SVD.</dd>
<dt><code>IncrementalPCA</code></dt>
<dd>Incremental Principal Component Analysis.</dd>
</dl>
<h2 id="references">References</h2>
<p>For n_components == 'mle', this class uses the method of <em>Minka, T. P.
"Automatic choice of dimensionality for PCA". In NIPS, pp. 598-604</em></p>
<p>Implements the probabilistic PCA model from:
Tipping, M. E., and Bishop, C. M. (1999). "Probabilistic principal
component analysis". Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 61(3), 611-622.
via the score and score_samples methods.
See <a href="http://www.miketipping.com/papers/met-mppca.pdf">http://www.miketipping.com/papers/met-mppca.pdf</a></p>
<p>For svd_solver == 'arpack', refer to <code>scipy.sparse.linalg.svds</code>.</p>
<p>For svd_solver == 'randomized', see:
<em>Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).
"Finding structure with randomness: Probabilistic algorithms for
constructing approximate matrix decompositions".
SIAM review, 53(2), 217-288.</em> and also
<em>Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).
"A randomized algorithm for the decomposition of matrices".
Applied and Computational Harmonic Analysis, 30(1), 47-68.</em></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.decomposition import PCA
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; pca = PCA(n_components=2)
&gt;&gt;&gt; pca.fit(X)
PCA(n_components=2)
&gt;&gt;&gt; print(pca.explained_variance_ratio_)
[0.9924... 0.0075...]
&gt;&gt;&gt; print(pca.singular_values_)
[6.30061... 0.54980...]
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; pca = PCA(n_components=2, svd_solver='full')
&gt;&gt;&gt; pca.fit(X)
PCA(n_components=2, svd_solver='full')
&gt;&gt;&gt; print(pca.explained_variance_ratio_)
[0.9924... 0.00755...]
&gt;&gt;&gt; print(pca.singular_values_)
[6.30061... 0.54980...]
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; pca = PCA(n_components=1, svd_solver='arpack')
&gt;&gt;&gt; pca.fit(X)
PCA(n_components=1, svd_solver='arpack')
&gt;&gt;&gt; print(pca.explained_variance_ratio_)
[0.99244...]
&gt;&gt;&gt; print(pca.singular_values_)
[6.30061...]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PCA(_PCA):

    symbol = &#39;pca&#39;
    full_name = &#39;Principal Component Analysis (PCA)&#39;

    def reduce(self, X):
        &#34;&#34;&#34;
        Project the input features onto a reduced space using principal
        component analysis.

        Parameters
        ----------
        X : numpy.ndarray
            Features in the original space.

        Returns
        -------
        numpy.ndarray
            Features in the reduced space.

        &#34;&#34;&#34;
        return self.fit_transform(X)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.decomposition._pca.PCA</li>
<li>sklearn.decomposition._base._BasePCA</li>
<li>sklearn.base.TransformerMixin</li>
<li>sklearn.base.BaseEstimator</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="partycls.dim_reduction.PCA.full_name"><code class="name">var <span class="ident">full_name</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="partycls.dim_reduction.PCA.symbol"><code class="name">var <span class="ident">symbol</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="partycls.dim_reduction.PCA.reduce"><code class="name flex">
<span>def <span class="ident">reduce</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Project the input features onto a reduced space using principal
component analysis.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Features in the original space.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Features in the reduced space.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reduce(self, X):
    &#34;&#34;&#34;
    Project the input features onto a reduced space using principal
    component analysis.

    Parameters
    ----------
    X : numpy.ndarray
        Features in the original space.

    Returns
    -------
    numpy.ndarray
        Features in the reduced space.

    &#34;&#34;&#34;
    return self.fit_transform(X)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="partycls.dim_reduction.TSNE"><code class="flex name class">
<span>class <span class="ident">TSNE</span></span>
<span>(</span><span>n_components=2, *, perplexity=30.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=1000, n_iter_without_progress=300, min_grad_norm=1e-07, metric='euclidean', init='random', verbose=0, random_state=None, method='barnes_hut', angle=0.5, n_jobs=None, square_distances='legacy')</span>
</code></dt>
<dd>
<div class="desc"><p>t-distributed Stochastic Neighbor Embedding.</p>
<p>t-SNE [1] is a tool to visualize high-dimensional data. It converts
similarities between data points to joint probabilities and tries
to minimize the Kullback-Leibler divergence between the joint
probabilities of the low-dimensional embedding and the
high-dimensional data. t-SNE has a cost function that is not convex,
i.e. with different initializations we can get different results.</p>
<p>It is highly recommended to use another dimensionality reduction
method (e.g. PCA for dense data or TruncatedSVD for sparse data)
to reduce the number of dimensions to a reasonable amount (e.g. 50)
if the number of features is very high. This will suppress some
noise and speed up the computation of pairwise distances between
samples. For more tips see Laurens van der Maaten's FAQ [2].</p>
<p>Read more in the :ref:<code>User Guide &lt;t_sne&gt;</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_components</code></strong> :&ensp;<code>int</code>, default=<code>2</code></dt>
<dd>Dimension of the embedded space.</dd>
<dt><strong><code>perplexity</code></strong> :&ensp;<code>float</code>, default=<code>30.0</code></dt>
<dd>The perplexity is related to the number of nearest neighbors that
is used in other manifold learning algorithms. Larger datasets
usually require a larger perplexity. Consider selecting a value
between 5 and 50. Different values can result in significantly
different results.</dd>
<dt><strong><code>early_exaggeration</code></strong> :&ensp;<code>float</code>, default=<code>12.0</code></dt>
<dd>Controls how tight natural clusters in the original space are in
the embedded space and how much space will be between them. For
larger values, the space between natural clusters will be larger
in the embedded space. Again, the choice of this parameter is not
very critical. If the cost function increases during initial
optimization, the early exaggeration factor or the learning rate
might be too high.</dd>
<dt><strong><code>learning_rate</code></strong> :&ensp;<code>float</code>, default=<code>200.0</code></dt>
<dd>The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If
the learning rate is too high, the data may look like a 'ball' with any
point approximately equidistant from its nearest neighbours. If the
learning rate is too low, most points may look compressed in a dense
cloud with few outliers. If the cost function gets stuck in a bad local
minimum increasing the learning rate may help.</dd>
<dt><strong><code>n_iter</code></strong> :&ensp;<code>int</code>, default=<code>1000</code></dt>
<dd>Maximum number of iterations for the optimization. Should be at
least 250.</dd>
<dt><strong><code>n_iter_without_progress</code></strong> :&ensp;<code>int</code>, default=<code>300</code></dt>
<dd>
<p>Maximum number of iterations without progress before we abort the
optimization, used after 250 initial iterations with early
exaggeration. Note that progress is only checked every 50 iterations so
this value is rounded to the next multiple of 50.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.17</p>
<p>parameter <em>n_iter_without_progress</em> to control stopping criteria.</p>
</div>
</dd>
<dt><strong><code>min_grad_norm</code></strong> :&ensp;<code>float</code>, default=<code>1e-7</code></dt>
<dd>If the gradient norm is below this threshold, the optimization will
be stopped.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>str</code> or <code>callable</code>, default=<code>'euclidean'</code></dt>
<dd>The metric to use when calculating distance between instances in a
feature array. If metric is a string, it must be one of the options
allowed by scipy.spatial.distance.pdist for its metric parameter, or
a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.
If metric is "precomputed", X is assumed to be a distance matrix.
Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays from X as input and return a value indicating
the distance between them. The default is "euclidean" which is
interpreted as squared euclidean distance.</dd>
<dt><strong><code>init</code></strong> :&ensp;<code>{'random', 'pca'}</code> or <code>ndarray</code> of <code>shape (n_samples, n_components)</code>,
default=<code>'random'</code></dt>
<dd>Initialization of embedding. Possible options are 'random', 'pca',
and a numpy array of shape (n_samples, n_components).
PCA initialization cannot be used with precomputed distances and is
usually more globally stable than random initialization.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code>, default=<code>0</code></dt>
<dd>Verbosity level.</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int, RandomState instance</code> or <code>None</code>, default=<code>None</code></dt>
<dd>Determines the random number generator. Pass an int for reproducible
results across multiple function calls. Note that different
initializations might result in different local minima of the cost
function. See :term: <code>Glossary &lt;random_state&gt;</code>.</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>str</code>, default=<code>'barnes_hut'</code></dt>
<dd>
<p>By default the gradient calculation algorithm uses Barnes-Hut
approximation running in O(NlogN) time. method='exact'
will run on the slower, but exact, algorithm in O(N^2) time. The
exact algorithm should be used when nearest-neighbor errors need
to be better than 3%. However, the exact method cannot scale to
millions of examples.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.17</p>
<p>Approximate optimization <em>method</em> via the Barnes-Hut.</p>
</div>
</dd>
<dt><strong><code>angle</code></strong> :&ensp;<code>float</code>, default=<code>0.5</code></dt>
<dd>Only used if method='barnes_hut'
This is the trade-off between speed and accuracy for Barnes-Hut T-SNE.
'angle' is the angular size (referred to as theta in [3]) of a distant
node as measured from a point. If this size is below 'angle' then it is
used as a summary node of all points contained within it.
This method is not very sensitive to changes in this parameter
in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing
computation time and angle greater 0.8 has quickly increasing error.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code>, default=<code>None</code></dt>
<dd>
<p>The number of parallel jobs to run for neighbors search. This parameter
has no impact when <code>metric="precomputed"</code> or
(<code>metric="euclidean"</code> and <code>method="exact"</code>).
<code>None</code> means 1 unless in a :obj:<code>joblib.parallel_backend</code> context.
<code>-1</code> means using all processors. See :term:<code>Glossary &lt;n_jobs&gt;</code>
for more details.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.22</p>
</div>
</dd>
<dt><strong><code>square_distances</code></strong> :&ensp;<code>True</code> or <code>'legacy'</code>, default=<code>'legacy'</code></dt>
<dd>
<p>Whether TSNE should square the distance values. <code>'legacy'</code> means
that distance values are squared only when <code>metric="euclidean"</code>.
<code>True</code> means that distance values are squared for all metrics.</p>
<div class="admonition versionadded">
<p class="admonition-title">Added in version:&ensp;0.24</p>
<p>Added to provide backward compatibility during deprecation of
legacy squaring behavior.</p>
</div>
<div class="admonition deprecated">
<p class="admonition-title">Deprecated since version:&ensp;0.24</p>
<p>Legacy squaring behavior was deprecated in 0.24. The <code>'legacy'</code>
value will be removed in 1.1 (renaming of 0.26), at which point the
default value will change to <code>True</code>.</p>
</div>
</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>embedding_</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_components)</code></dt>
<dd>Stores the embedding vectors.</dd>
<dt><strong><code>kl_divergence_</code></strong> :&ensp;<code>float</code></dt>
<dd>Kullback-Leibler divergence after optimization.</dd>
<dt><strong><code>n_iter_</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of iterations run.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.manifold import TSNE
&gt;&gt;&gt; X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
&gt;&gt;&gt; X_embedded = TSNE(n_components=2).fit_transform(X)
&gt;&gt;&gt; X_embedded.shape
(4, 2)
</code></pre>
<h2 id="references">References</h2>
<p>[1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data
Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.</p>
<p>[2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding
<a href="https://lvdmaaten.github.io/tsne/">https://lvdmaaten.github.io/tsne/</a></p>
<p>[3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.
Journal of Machine Learning Research 15(Oct):3221-3245, 2014.
<a href="https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf">https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TSNE(_TSNE):

    symbol = &#39;tsne&#39;
    full_name = &#39;t-distributed Stochastic Neighbor Embedding (t-SNE)&#39;

    def reduce(self, X):
        &#34;&#34;&#34;
        Project the input features onto a reduced space using t-distributed 
        stochastic neighbor embedding.

        Parameters
        ----------
        X : numpy.ndarray
            Features in the original space.

        Returns
        -------
        numpy.ndarray
            Features in the reduced space.

        &#34;&#34;&#34;
        return self.fit_transform(X)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.manifold._t_sne.TSNE</li>
<li>sklearn.base.BaseEstimator</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="partycls.dim_reduction.TSNE.full_name"><code class="name">var <span class="ident">full_name</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="partycls.dim_reduction.TSNE.symbol"><code class="name">var <span class="ident">symbol</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="partycls.dim_reduction.TSNE.reduce"><code class="name flex">
<span>def <span class="ident">reduce</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Project the input features onto a reduced space using t-distributed
stochastic neighbor embedding.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Features in the original space.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Features in the reduced space.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reduce(self, X):
    &#34;&#34;&#34;
    Project the input features onto a reduced space using t-distributed 
    stochastic neighbor embedding.

    Parameters
    ----------
    X : numpy.ndarray
        Features in the original space.

    Returns
    -------
    numpy.ndarray
        Features in the reduced space.

    &#34;&#34;&#34;
    return self.fit_transform(X)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="Home" href="https://htmlpreview.github.io/?https://github.com/jorisparet/partycls/blob/master/docs/partycls/index.html">
<img class="logo" src="https://raw.githubusercontent.com/jorisparet/partycls/jupyter-book/logo/logo.svg" alt="Logo">
</a>
</header>
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search docs" aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<!-- joris: before <h1>Index</h1> -->
<h1><a href="index.html">Index</a></h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="partycls" href="index.html">partycls</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="partycls.dim_reduction.AutoEncoder" href="#partycls.dim_reduction.AutoEncoder">AutoEncoder</a></code></h4>
<ul class="">
<li><code><a title="partycls.dim_reduction.AutoEncoder.full_name" href="#partycls.dim_reduction.AutoEncoder.full_name">full_name</a></code></li>
<li><code><a title="partycls.dim_reduction.AutoEncoder.n_components" href="#partycls.dim_reduction.AutoEncoder.n_components">n_components</a></code></li>
<li><code><a title="partycls.dim_reduction.AutoEncoder.reduce" href="#partycls.dim_reduction.AutoEncoder.reduce">reduce</a></code></li>
<li><code><a title="partycls.dim_reduction.AutoEncoder.symbol" href="#partycls.dim_reduction.AutoEncoder.symbol">symbol</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="partycls.dim_reduction.LocallyLinearEmbedding" href="#partycls.dim_reduction.LocallyLinearEmbedding">LocallyLinearEmbedding</a></code></h4>
<ul class="">
<li><code><a title="partycls.dim_reduction.LocallyLinearEmbedding.full_name" href="#partycls.dim_reduction.LocallyLinearEmbedding.full_name">full_name</a></code></li>
<li><code><a title="partycls.dim_reduction.LocallyLinearEmbedding.reduce" href="#partycls.dim_reduction.LocallyLinearEmbedding.reduce">reduce</a></code></li>
<li><code><a title="partycls.dim_reduction.LocallyLinearEmbedding.symbol" href="#partycls.dim_reduction.LocallyLinearEmbedding.symbol">symbol</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="partycls.dim_reduction.PCA" href="#partycls.dim_reduction.PCA">PCA</a></code></h4>
<ul class="">
<li><code><a title="partycls.dim_reduction.PCA.full_name" href="#partycls.dim_reduction.PCA.full_name">full_name</a></code></li>
<li><code><a title="partycls.dim_reduction.PCA.reduce" href="#partycls.dim_reduction.PCA.reduce">reduce</a></code></li>
<li><code><a title="partycls.dim_reduction.PCA.symbol" href="#partycls.dim_reduction.PCA.symbol">symbol</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="partycls.dim_reduction.TSNE" href="#partycls.dim_reduction.TSNE">TSNE</a></code></h4>
<ul class="">
<li><code><a title="partycls.dim_reduction.TSNE.full_name" href="#partycls.dim_reduction.TSNE.full_name">full_name</a></code></li>
<li><code><a title="partycls.dim_reduction.TSNE.reduce" href="#partycls.dim_reduction.TSNE.reduce">reduce</a></code></li>
<li><code><a title="partycls.dim_reduction.TSNE.symbol" href="#partycls.dim_reduction.TSNE.symbol">symbol</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
By Joris Paret and Daniele Coslovich. © Copyright 2021.
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>